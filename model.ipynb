{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimhatem12/Speech-Emotion-Recognition-/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JspCH3lVUuUG"
      },
      "source": [
        "# **Speech Emotion Recogition (Classification) in real-time using Deep LSTM layers**\n",
        "### ***A Deep Learning LSTM based model with keras.***\n",
        "---\n",
        "\n",
        "### Final project (B.Sc. requirement)  \n",
        "Development by **Karim hatem hamed.**\n",
        "\n",
        "Instructor: **Dr. Eslam Elshaarawy**\n",
        "\n",
        "Computer Science.\n",
        "\n",
        "MSA Universty , Egypt.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4RcjQMCV89B"
      },
      "source": [
        "# **LIBRARIES & GOOGLE AUTH**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nYS9qahzAQ_",
        "outputId": "31158c07-beea-4e6b-ac21-3a33ab6490c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K31C-zTfZdFa"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        "!pip install pydub\n",
        "!pip install pywt\n",
        "!pip install noisereduce\n",
        "!pip install json-tricks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li2EfZXmQehM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "from json_tricks import dump, load\n",
        "\n",
        "from pydub import AudioSegment, effects\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "import pywt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUMWt35lTQQL"
      },
      "outputs": [],
      "source": [
        "# Emotion kind validation function for TESS database, due to emotions written within the file names.\n",
        "def find_emotion_T(name): \n",
        "        if('neutral' in name): return \"01\"\n",
        "        elif('NEU' in name): return \"01\"\n",
        "        elif('happy' in name): return \"03\"\n",
        "        elif('HAP' in name): return \"03\"\n",
        "        elif('sad' in name): return \"04\"\n",
        "        elif('SAD' in name): return \"04\"\n",
        "        elif('angry' in name): return \"05\"\n",
        "        elif('ANG' in name): return \"05\"\n",
        "        elif('fear' in name): return \"06\"\n",
        "        elif('FEA' in name): return \"06\"\n",
        "        elif('disgust' in name): return \"07\"\n",
        "        elif('DIS' in name): return \"07\"\n",
        "        elif('ps' in name): return \"08\"\n",
        "        else: return \"-1\"\n",
        "        \n",
        " \n",
        "# 'emotions' list fix for classification purposes:\n",
        "#     Classification values start from 0, Thus an 'n = n-1' operation has been executed for both RAVDESS and TESS databases:\n",
        "def emotionfix(e_num):\n",
        "        if e_num == \"01\":   return 0 # neutral\n",
        "        #elif e_num == \"02\": return 1 # calm\n",
        "        elif e_num == \"03\": return 1 # happy\n",
        "        elif e_num == \"04\": return 2 # sad\n",
        "        elif e_num == \"05\": return 3 # angry\n",
        "        elif e_num == \"06\": return 4 # fear\n",
        "        elif e_num == \"07\": return 5 # disgust\n",
        "        else:               return 6 # suprised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydC8v5SNra-0",
        "outputId": "60025050-a835-47f2-80bd-19edc90425f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sample length: 204288\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Maximum samples count for padding purposes.\n",
        "\n",
        "sample_lengths = []\n",
        "# folder_path = '/content/drive/MyDrive/Colab_Notebooks/AudioFiles/TESS'\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/AudioFiles/Used Dataset'\n",
        "\n",
        "for subdir, dirs, files in os.walk(folder_path):\n",
        "  for file in files: \n",
        "    x, sr = librosa.load(path = os.path.join(subdir,file), sr = None)\n",
        "    xt, index = librosa.effects.trim(x, top_db=30)\n",
        "     \n",
        "    sample_lengths.append(len(xt))\n",
        "\n",
        "print('Maximum sample length:', np.max(sample_lengths))               \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHsU4Noi1n_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18db38b1-311c-464d-bcb1-986e85c7bd70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running time: 21.0543 minutes\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "tic = time.perf_counter()\n",
        "\n",
        "# Initialize data lists\n",
        "rms = []\n",
        "zcr = []\n",
        "mfcc = []\n",
        "chroma = []\n",
        "emotions = []\n",
        "\n",
        "# Initialize variables\n",
        "total_length = 228864 #228864  #305152  #5005152    # desired frame length for all of the audio samples.\n",
        "frame_length = 2048\n",
        "hop_length = 512\n",
        "\n",
        "# folder_path = '/content/drive/MyDrive/Colab_Notebooks/AudioFiles/TESS' \n",
        "folder_path = '/content/drive/MyDrive/AudioFiles/Used Dataset'\n",
        "\n",
        "for subdir, dirs, files in os.walk(folder_path):\n",
        "  for file in files: \n",
        "    # Fetch the sample rate.\n",
        "      _, sr = librosa.load(path = os.path.join(subdir,file), sr = None) # sr (the sample rate) is used for librosa's MFCCs. '_' is irrelevant.\n",
        "    # Load the audio file.\n",
        "      rawsound = AudioSegment.from_file(os.path.join(subdir,file)) \n",
        "    # Normalize the audio to +5.0 dBFS.\n",
        "      normalizedsound = effects.normalize(rawsound, headroom = 5.0) \n",
        "    # Transform the normalized audio to np.array of samples.\n",
        "      normal_x = np.array(normalizedsound.get_array_of_samples(), dtype = 'float32')\n",
        "    # Trim silence from the beginning and the end.\n",
        "      xt,  index = librosa.effects.trim(normal_x, top_db=30)\n",
        "    # Pad for duration equalization. \n",
        "      padded_x = np.pad(xt, (0, total_length-len(xt)), 'constant')\n",
        "    # Noise reduction.\n",
        "      final_x = nr.reduce_noise(y=padded_x,y_noise=padded_x, sr=sr)\n",
        "\n",
        "   # Features extraction \n",
        "      f1 = librosa.feature.rms(final_x, frame_length=frame_length, hop_length=hop_length) # Energy - Root Mean Square   \n",
        "      f2 = librosa.feature.zero_crossing_rate(final_x , frame_length=frame_length, hop_length=hop_length, center=True) # ZCR      \n",
        "      f3 = librosa.feature.mfcc(final_x, sr=sr, n_mfcc=13, hop_length = hop_length) # MFCC\n",
        "      f4 = librosa.feature.chroma_stft(final_x,sr=sr) #chroma\n",
        "      \n",
        "   # Emotion extraction from the different databases\n",
        "      if (find_emotion_T(file) != \"-1\"): #TESS database validation\n",
        "            name = find_emotion_T(file)\n",
        "            \n",
        "      else:                              #RAVDESS database validation\n",
        "            name = file[6:8]\n",
        "                                \n",
        "\n",
        "   # Filling the data lists  \n",
        "      rms.append(f1)\n",
        "      zcr.append(f2)\n",
        "      mfcc.append(f3)\n",
        "      chroma.append(f4)\n",
        "      emotions.append(emotionfix(name)) \n",
        "      \n",
        "\n",
        "toc = time.perf_counter()\n",
        "print(f\"Running time: {(toc - tic)/60:0.4f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFP_FOZsp6qO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ccc813-d715-4e03-e0ec-04948935afb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMS shape: (4996, 448, 1)\n",
            "ZCR shape: (4996, 448, 1)\n",
            "MFCCs shape: (4996, 448, 13)\n",
            "Chroma shape: (4996, 448, 12)\n"
          ]
        }
      ],
      "source": [
        "# Adjusting features shape to the 3D format: (batch, timesteps, feature)\n",
        "\n",
        "f_rms = np.asarray(rms).astype('float32')\n",
        "f_rms = np.swapaxes(f_rms,1,2)\n",
        "f_zcr = np.asarray(zcr).astype('float32')\n",
        "f_zcr = np.swapaxes(f_zcr,1,2)\n",
        "f_mfccs = np.asarray(mfcc).astype('float32')\n",
        "f_mfccs = np.swapaxes(f_mfccs,1,2)\n",
        "f_chroma = np.asarray(chroma).astype('float32')\n",
        "f_chroma = np.swapaxes(f_chroma,1,2)\n",
        "\n",
        "print('RMS shape:',f_rms.shape)\n",
        "print('ZCR shape:',f_zcr.shape)\n",
        "print('MFCCs shape:',f_mfccs.shape)\n",
        "print('Chroma shape:',f_chroma.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSpXqgJqMHvR"
      },
      "outputs": [],
      "source": [
        "# Concatenating all features to 'X' variable.\n",
        "X = np.concatenate(( f_rms,f_zcr,f_mfccs,  f_chroma), axis=2) #,\n",
        "# Preparing 'Y' as a 2D shaped variable.\n",
        "Y = np.asarray(emotions).astype('int8')\n",
        "Y = np.expand_dims(Y, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZR2SoDp61pR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "3b437943-7bdf-4d57-982a-1538f2707106"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[[6], [4], [2], [4], [1], [6], [2], [6], [1], [5], [4], [1], [4], [0], [3], [5], [3], [1], [0], [6], [6], [4], [3], [5], [2], [5], [3], [0], [2], [4], [6], [5], [1], [2], [3], [4], [0], [2], [1], [6], [0], [2], [6], [1], [4], [0], [1], [3], [5], [4], [5], [3], [5], [0], [4], [5], [3], [1], [6], [3], [2], [6], [2], [0], [4], [1], [1], [0], [5], [3], [6], [0], [4], [2], [6], [3], [2], [4], [5], [3], [4], [5], [0], [6], [2], [1], [4], [6], [3], [2], [0], [3], [6], [5], [1], [0], [2], [1], [4], [5], [3], [0], [6], [2], [0], [1], [1], [4], [5], [2], [0], [5], [3], [4], [5], [6], [1], [3], [6], [2], [4], [2], [6], [6], [0], [3], [4], [1], [0], [3], [5], [2], [1], [5], [6], [0], [1], [4], [2], [5], [1], [4], [3], [0], [6], [5], [2], [1], [4], [5], [2], [3], [6], [3], [4], [0], [1], [5], [6], [0], [4], [5], [2], [3], [2], [0], [6], [1], [3], [1], [6], [1], [6], [0], [4], [5], [3], [2], [0], [4], [1], [0], [5], [1], [5], [3], [3], [6], [4], [4], [2], [2], [3], [5], [4], [0], [2], [6], [6], [2], [3], [1], [5], [0], [5], [2], [3], [6], [6], [0], [1], [4], [2], [0], [4], [1], [4], [3], [5], [1], [3], [2], [1], [6], [5], [4], [0], [1], [5], [5], [4], [6], [3], [2], [6], [0], [3], [0], [2], [4], [2], [0], [3], [2], [4], [5], [6], [0], [1], [1], [6], [3], [1], [4], [5], [0], [2], [4], [1], [6], [5], [3], [0], [5], [3], [0], [6], [6], [1], [5], [2], [4], [2], [3], [2], [1], [3], [5], [0], [0], [4], [6], [4], [6], [1], [4], [1], [1], [5], [4], [3], [5], [3], [6], [0], [2], [2], [0], [5], [3], [0], [6], [4], [3], [5], [6], [1], [2], [2], [4], [2], [3], [4], [1], [0], [0], [5], [2], [6], [1], [6], [3], [5], [0], [1], [5], [4], [1], [6], [3], [0], [2], [4], [5], [3], [4], [6], [1], [5], [3], [0], [2], [2], [6], [0], [1], [2], [2], [6], [0], [3], [1], [6], [5], [4], [4], [3], [4], [1], [5], [3], [5], [4], [0], [6], [2], [1], [3], [0], [6], [3], [5], [1], [0], [2], [2], [6], [4], [6], [4], [4], [1], [2], [3], [1], [0], [0], [5], [5], [2], [3], [6], [6], [2], [3], [5], [0], [1], [5], [4], [4], [3], [6], [0], [0], [5], [1], [1], [2], [2], [6], [4], [4], [3], [3], [6], [0], [2], [4], [1], [5], [1], [5], [5], [3], [2], [3], [1], [0], [2], [4], [6], [0], [5], [6], [0], [1], [4], [0], [1], [6], [3], [6], [2], [5], [4], [6], [2], [5], [3], [5], [0], [3], [2], [4], [1], [4], [1], [6], [2], [3], [1], [2], [4], [5], [3], [0], [6], [0], [3], [5], [0], [0], [2], [4], [4], [1], [6], [1], [5], [0], [4], [5], [6], [2], [2], [1], [3], [5], [6], [3], [3], [6], [1], [6], [4], [1], [5], [0], [2], [0], [4], [3], [2], [2], [5], [4], [1], [6], [3], [5], [4], [1], [0], [3], [1], [5], [0], [0], [3], [6], [2], [5], [2], [4], [6], [1], [0], [4], [5], [6], [3], [1], [4], [0], [2], [5], [1], [5], [6], [0], [3], [3], [6], [4], [4], [2], [2], [4], [1], [2], [3], [6], [1], [6], [0], [5], [0], [2], [1], [2], [6], [5], [4], [5], [3], [0], [3], [4], [1], [0], [5], [3], [2], [5], [2], [1], [6], [6], [3], [0], [4], [1], [4], [2], [0], [4], [3], [1], [0], [5], [2], [6], [6], [4], [0], [1], [4], [5], [3], [2], [5], [3], [6], [1], [0], [6], [4], [1], [3], [5], [2], [0], [3], [6], [5], [2], [1], [6], [4], [0], [6], [1], [5], [4], [2], [3], [0], [0], [2], [3], [2], [6], [4], [5], [3], [1], [1], [5], [4], [3], [2], [1], [3], [2], [5], [5], [6], [4], [0], [0], [6], [4], [4], [5], [2], [2], [0], [6], [1], [3], [0], [6], [1], [1], [3], [4], [4], [0], [1], [5], [6], [5], [3], [2], [0], [6], [5], [3], [2], [6], [4], [0], [5], [2], [3], [1], [0], [1], [3], [1], [2], [0], [6], [5], [4], [2], [4], [6], [3], [2], [1], [3], [5], [5], [0], [4], [1], [4], [6], [0], [5], [4], [5], [6], [3], [2], [6], [1], [2], [4], [3], [0], [2], [4], [0], [0], [6], [5], [1], [6], [1], [2], [3], [5], [6], [5], [0], [0], [1], [2], [1], [4], [3], [4], [3], [5], [4], [3], [4], [6], [5], [1], [6], [0], [2], [3], [2], [2], [3], [6], [6], [0], [4], [1], [0], [3], [1], [2], [5], [3], [1], [1], [5], [6], [4], [4], [0], [2], [5], [0], [6], [1], [0], [2], [2], [3], [3], [5], [5], [6], [4], [3], [4], [4], [2], [2], [6], [0], [3], [6], [0], [1], [1], [5], [4], [3], [5], [5], [0], [2], [6], [1], [0], [1], [4], [6], [5], [2], [5], [4], [2], [3], [0], [4], [3], [6], [1], [1], [1], [5], [3], [2], [2], [0], [3], [4], [5], [6], [0], [6], [0], [6], [2], [4], [4], [5], [3], [0], [1], [6], [1], [4], [0], [2], [3], [2], [4], [1], [6], [5], [3], [5], [5], [3], [0], [6], [3], [0], [6], [1], [2], [1], [2], [4], [0], [6], [4], [3], [4], [0], [5], [1], [5], [2], [6], [1], [6], [1], [4], [2], [2], [0], [5], [3], [5], [3], [4], [1], [6], [2], [1], [0], [3], [4], [6], [2], [0], [5], [3], [3], [5], [4], [1], [1], [2], [4], [0], [5], [6], [0], [4], [1], [2], [5], [0], [5], [3], [6], [3], [2], [6], [6], [1], [4], [5], [6], [0], [2], [4], [2], [3], [3], [0], [1], [6], [1], [6], [4], [0], [2], [1], [5], [4], [5], [3], [0], [2], [4], [3], [4], [0], [6], [1], [1], [5], [3], [2], [5], [1], [0], [2], [4], [0], [6], [3], [6], [5], [3], [2], [2], [6], [4], [5], [4], [3], [6], [0], [1], [0], [5], [1], [0], [3], [4], [5], [2], [6], [1], [2], [5], [0], [1], [5], [0], [2], [3], [4], [2], [1], [3], [5], [4], [6], [3], [1], [6], [0], [2], [0], [5], [6], [1], [2], [4], [0], [1], [4], [6], [3], [4], [3], [1], [2], [5], [0], [5], [5], [3], [0], [6], [3], [5], [6], [4], [2], [1], [2], [4], [0], [3], [6], [0], [6], [1], [2], [2], [4], [5], [1], [0], [3], [1], [0], [6], [3], [2], [5], [1], [4], [4], [5], [2], [0], [6], [5], [3], [1], [6], [4], [3], [2], [5], [6], [5], [4], [1], [2], [6], [0], [4], [3], [0], [1], [1], [2], [2], [6], [4], [3], [5], [1], [5], [0], [3], [4], [5], [0], [0], [4], [2], [6], [2], [3], [1], [6], [3], [5], [0], [6], [4], [6], [3], [0], [1], [4], [1], [2], [5], [2], [1], [0], [5], [1], [6], [0], [5], [4], [3], [3], [4], [2], [1], [5], [4], [3], [1], [2], [5], [4], [0], [2], [6], [6], [3], [3], [6], [2], [0], [3], [1], [0], [5], [2], [4], [6], [2], [6], [5], [5], [0], [3], [1], [0], [4], [1], [4], [3], [1], [5], [2], [6], [3], [4], [5], [0], [6], [4], [2], [0], [3], [6], [2], [1], [0], [3], [1], [5], [4], [6], [2], [5], [1], [3], [4], [2], [1], [6], [0], [0], [4], [5], [3], [6], [2], [6], [0], [5], [1], [4], [3], [5], [4], [2], [2], [2], [0], [1], [1], [0], [3], [5], [6], [4], [6], [6], [4], [4], [0], [3], [0], [5], [1], [5], [6], [3], [1], [2], [5], [6], [4], [3], [0], [5], [2], [1], [2], [4], [3], [4], [0], [2], [1], [2], [1], [0], [5], [6], [6], [3], [6], [5], [4], [3], [4], [1], [2], [0], [5], [3], [0], [1], [2], [6], [5], [6], [3], [4], [1], [0], [2], [5], [4], [3], [1], [0], [3], [6], [3], [6], [2], [4], [2], [0], [5], [1], [5], [4], [5], [6], [1], [2], [6], [3], [1], [4], [0], [0], [4], [2], [0], [3], [6], [1], [4], [1], [5], [2], [3], [5], [5], [0], [2], [6], [1], [6], [4], [3], [0], [5], [2], [3], [2], [6], [0], [5], [2], [3], [0], [6], [4], [1], [4], [1], [5], [4], [1], [3], [1], [2], [0], [0], [6], [3], [5], [4], [4], [3], [0], [6], [2], [2], [5], [5], [3], [6], [1], [6], [3], [0], [0], [4], [1], [5], [2], [4], [6], [2], [1], [4], [5], [1], [0], [6], [5], [2], [4], [1], [3], [3], [0], [0], [2], [3], [6], [6], [4], [1], [3], [5], [5], [2], [1], [2], [4], [6], [1], [3], [5], [2], [0], [6], [0], [4], [1], [4], [3], [4], [5], [2], [3], [1], [0], [5], [6], [2], [5], [1], [0], [3], [5], [2], [4], [6], [3], [0], [6], [5], [1], [4], [3], [1], [0], [4], [2], [6], [0], [6], [2], [4], [4], [5], [1], [0], [1], [5], [3], [2], [3], [6], [0], [0], [5], [1], [2], [6], [4], [2], [5], [3], [6], [3], [1], [3], [4], [2], [2], [5], [6], [1], [6], [4], [0], [0], [4], [5], [4], [2], [0], [6], [1], [5], [3], [1], [3], [6], [4], [1], [0], [3], [2], [5], [6], [2], [5], [0], [3], [6], [4], [6], [0], [0], [2], [5], [3], [4], [1], [1], [2], [3], [2], [3], [5], [4], [1], [0], [6], [1], [4], [5], [6], [3], [5], [4], [5], [0], [1], [2], [2], [0], [3], [6], [0], [6], [1], [4], [4], [2], [6], [5], [2], [0], [3], [1], [1], [1], [5], [0], [3], [4], [2], [5], [3], [4], [6], [0], [6], [1], [0], [5], [4], [2], [5], [3], [3], [2], [6], [1], [4], [0], [6], [4], [0], [5], [1], [2], [2], [6], [3], [0], [1], [3], [4], [3], [6], [0], [5], [2], [5], [1], [4], [3], [2], [6], [5], [1], [0], [5], [3], [0], [4], [6], [2], [6], [4], [6], [1], [1], [3], [5], [2], [0], [0], [4], [5], [4], [6], [0], [4], [5], [3], [2], [2], [3], [1], [1], [0], [2], [0], [3], [6], [5], [6], [2], [1], [3], [4], [5], [6], [4], [5], [6], [1], [0], [3], [2], [1], [0], [4], [3], [2], [6], [1], [4], [0], [1], [5], [5], [2], [4], [3], [6], [5], [6], [3], [2], [4], [1], [2], [0], [0], [5], [3], [2], [5], [6], [0], [0], [3], [1], [4], [6], [4], [1], [4], [5], [5], [3], [3], [4], [1], [2], [6], [0], [1], [2], [1], [5], [2], [3], [6], [3], [2], [0], [0], [4], [6], [3], [0], [5], [6], [4], [4], [1], [0], [5], [1], [2], [0], [5], [6], [6], [5], [4], [3], [2], [2], [3], [1], [4], [1], [4], [6], [6], [0], [5], [2], [1], [0], [3], [2], [3], [1], [5], [5], [3], [4], [2], [4], [1], [0], [6], [6], [5], [1], [3], [2], [0], [3], [2], [4], [5], [6], [0], [6], [0], [0], [2], [4], [3], [4], [5], [1], [6], [1], [1], [2], [4], [0], [5], [2], [3], [1], [3], [6], [5], [4], [6], [6], [4], [0], [3], [1], [2], [0], [5], [2], [3], [0], [0], [1], [3], [2], [6], [5], [5], [4], [4], [6], [1], [3], [4], [5], [2], [6], [2], [4], [3], [1], [0], [1], [5], [3], [2], [4], [5], [3], [1], [0], [6], [2], [0], [6], [5], [4], [5], [3], [0], [0], [6], [1], [4], [6], [2], [1], [0], [0], [2], [3], [4], [1], [6], [2], [4], [1], [5], [3], [5], [2], [6], [3], [4], [0], [4], [2], [6], [5], [1], [3], [5], [3], [2], [0], [6], [1], [4], [5], [1], [3], [0], [6], [2], [0], [1], [2], [6], [4], [1], [0], [5], [5], [4], [3], [1], [3], [5], [3], [4], [0], [4], [2], [2], [5], [6], [6], [5], [2], [1], [1], [3], [0], [4], [2], [0], [6], [6], [1], [0], [1], [4], [5], [4], [2], [3], [6], [3], [5], [3], [6], [6], [5], [0], [2], [4], [2], [3], [1], [0], [1], [6], [4], [6], [5], [1], [0], [2], [0], [3], [4], [5], [4], [4], [5], [1], [2], [6], [2], [0], [5], [3], [3], [2], [0], [4], [1], [6], [3], [6], [2], [5], [0], [5], [1], [3], [2], [0], [0], [3], [1], [5], [3], [4], [4], [1], [1], [4], [1], [3], [5], [1], [4], [2], [0], [2], [0], [3], [3], [3], [2], [5], [3], [4], [5], [3], [4], [1], [5], [0], [1], [3], [2], [5], [4], [1], [1], [2], [2], [0], [0], [0], [4], [0], [4], [1], [3], [1], [5], [5], [5], [2], [3], [5], [5], [1], [4], [5], [1], [2], [5], [2], [1], [5], [0], [1], [4], [0], [2], [5], [5], [3], [4], [3], [3], [3], [0], [1], [3], [3], [3], [1], [3], [2], [4], [2], [1], [1], [4], [3], [5], [0], [5], [2], [2], [3], [4], [1], [1], [1], [0], [0], [3], [4], [3], [5], [5], [4], [1], [5], [2], [2], [1], [2], [5], [3], [3], [3], [3], [5], [2], [2], [1], [4], [5], [4], [2], [1], [0], [4], [1], [4], [5], [2], [5], [4], [0], [2], [1], [0], [0], [5], [2], [4], [0], [5], [2], [0], [1], [4], [3], [2], [1], [2], [3], [1], [2], [3], [3], [5], [0], [3], [4], [2], [0], [4], [0], [5], [1], [1], [3], [3], [3], [2], [2], [0], [0], [3], [3], [1], [4], [2], [2], [2], [0], [3], [4], [0], [0], [5], [4], [5], [0], [4], [5], [1], [4], [5], [2], [1], [5], [0], [3], [0], [5], [1], [4], [2], [0], [3], [4], [6], [5], [2], [4], [5], [1], [3], [6], [0], [2], [3], [6], [0], [4], [6], [5], [1], [1], [2], [3], [1], [1], [2], [5], [4], [0], [6], [4], [3], [0], [5], [6], [2], [3], [4], [2], [6], [5], [1], [5], [4], [3], [4], [2], [3], [3], [0], [6], [6], [1], [0], [2], [5], [1], [5], [6], [4], [0], [3], [5], [6], [0], [1], [2], [1], [4], [1], [2], [3], [1], [5], [0], [3], [6], [4], [4], [5], [2], [3], [4], [0], [2], [6], [3], [1], [0], [5], [5], [2], [4], [6], [5], [3], [1], [2], [2], [0], [0], [1], [6], [6], [4], [3], [4], [0], [2], [4], [1], [3], [5], [5], [1], [6], [6], [5], [2], [2], [3], [1], [0], [5], [0], [4], [3], [6], [3], [0], [2], [5], [6], [1], [6], [4], [1], [0], [4], [5], [2], [6], [3], [3], [0], [4], [2], [4], [1], [1], [5], [0], [2], [5], [5], [3], [2], [3], [1], [6], [4], [6], [0], [4], [0], [6], [0], [5], [3], [4], [1], [2], [1], [6], [2], [3], [2], [1], [3], [4], [5], [4], [0], [1], [6], [5], [0], [6], [1], [2], [5], [4], [6], [5], [3], [3], [4], [2], [0], [4], [0], [5], [2], [3], [6], [0], [1], [2], [6], [3], [1], [4], [5], [4], [6], [1], [5], [3], [1], [0], [6], [0], [2], [5], [4], [1], [3], [3], [6], [5], [2], [2], [4], [0], [1], [3], [1], [5], [6], [0], [6], [2], [0], [4], [2], [1], [1], [3], [6], [3], [2], [4], [5], [5], [0], [0], [4], [2], [6], [4], [3], [1], [3], [2], [6], [0], [5], [4], [5], [6], [0], [5], [0], [1], [4], [3], [1], [3], [2], [2], [6], [1], [4], [0], [3], [1], [4], [5], [2], [0], [5], [6], [6], [1], [0], [5], [2], [2], [5], [3], [4], [3], [1], [4], [6], [2], [6], [0], [3], [5], [1], [3], [5], [4], [2], [6], [0], [1], [1], [2], [2], [0], [5], [6], [3], [0], [6], [4], [4], [5], [3], [5], [6], [4], [1], [1], [2], [4], [0], [3], [4], [3], [5], [6], [6], [0], [5], [0], [1], [3], [2], [2], [3], [4], [6], [6], [2], [0], [5], [1], [0], [4], [2], [1], [5], [0], [4], [0], [1], [3], [4], [1], [2], [6], [5], [3], [5], [2], [6], [4], [5], [6], [3], [2], [3], [1], [0], [0], [3], [6], [4], [2], [4], [2], [1], [5], [1], [6], [0], [5], [2], [3], [0], [0], [1], [5], [4], [4], [3], [6], [1], [1], [4], [6], [2], [3], [4], [0], [5], [6], [3], [2], [5], [2], [6], [1], [3], [2], [0], [4], [6], [5], [1], [3], [0], [5], [1], [0], [1], [6], [2], [4], [3], [5], [4], [0], [0], [2], [6], [3], [4], [3], [5], [5], [6], [1], [2], [0], [0], [6], [5], [6], [2], [4], [1], [1], [4], [2], [3], [3], [5], [2], [0], [4], [3], [4], [5], [1], [6], [0], [1], [6], [3], [5], [2], [4], [5], [3], [1], [6], [2], [4], [0], [2], [6], [2], [1], [4], [0], [0], [5], [3], [1], [6], [4], [6], [1], [0], [4], [3], [2], [6], [1], [3], [5], [0], [5], [3], [4], [6], [3], [5], [1], [2], [2], [5], [4], [0], [0], [1], [2], [5], [0], [6], [2], [1], [6], [4], [3], [4], [4], [5], [3], [6], [5], [1], [0], [1], [3], [0], [2], [1], [5], [6], [3], [5], [2], [6], [4], [0], [2], [3], [0], [0], [2], [3], [2], [6], [4], [6], [1], [4], [1], [5], [6], [3], [1], [5], [4], [2], [1], [5], [0], [4], [3], [3], [6], [0], [5], [2], [2], [0], [1], [3], [5], [6], [4], [1], [6], [0], [3], [2], [4], [1], [5], [2], [4], [6], [0], [0], [1], [4], [1], [5], [3], [4], [6], [3], [2], [0], [5], [4], [5], [3], [2], [3], [6], [1], [2], [0], [5], [6], [0], [3], [2], [6], [4], [4], [0], [2], [6], [1], [5], [1], [3], [3], [4], [5], [4], [0], [6], [5], [1], [0], [2], [1], [3], [5], [1], [6], [2], [3], [4], [0], [6], [5], [4], [2], [2], [3], [0], [1], [2], [6], [6], [1], [3], [0], [5], [4], [0], [6], [0], [5], [2], [3], [5], [6], [1], [4], [4], [1], [3], [3], [2], [1], [0], [5], [5], [4], [6], [2], [1], [4], [5], [2], [2], [0], [1], [3], [5], [6], [6], [3], [0], [4], [0], [6], [4], [6], [1], [3], [2], [5], [4], [0], [1], [1], [5], [3], [2], [6], [5], [2], [0], [1], [3], [4], [4], [2], [1], [5], [0], [2], [6], [3], [4], [0], [3], [6], [5], [6], [0], [3], [4], [4], [2], [6], [5], [1], [1], [0], [1], [3], [4], [2], [3], [1], [4], [2], [6], [0], [5], [5], [5], [2], [4], [6], [6], [0], [2], [1], [0], [3], [5], [3], [4], [1], [0], [2], [3], [6], [4], [6], [2], [3], [5], [2], [2], [6], [3], [5], [2], [4], [4], [6], [3], [5], [3], [4], [4], [4], [6], [5], [4], [6], [4], [1], [2], [3], [1], [5], [6], [2], [6], [3], [1], [5], [6], [2], [6], [1], [3], [2], [6], [5], [3], [6], [1], [4], [2], [4], [3], [5], [6], [3], [4], [2], [3], [6], [2], [4], [2], [4], [2], [3], [1], [6], [4], [2], [3], [4], [6], [1], [6], [4], [3], [6], [5], [6], [5], [6], [5], [6], [6], [5], [5], [6], [5], [6], [6], [5], [3], [5], [4], [4], [1], [4], [3], [0], [2], [5], [1], [0], [3], [3], [1], [4], [1], [2], [5], [4], [3], [3], [5], [3], [0], [1], [0], [1], [2], [2], [3], [4], [4], [2], [2], [0], [5], [1], [5], [2], [1], [5], [3], [3], [2], [5], [4], [1], [1], [0], [2], [0], [1], [4], [0], [5], [3], [4], [4], [0], [5], [2], [2], [0], [1], [4], [3], [4], [5], [2], [5], [3], [1], [0], [5], [4], [2], [3], [2], [4], [0], [1], [3], [1], [5], [3], [2], [0], [4], [3], [5], [4], [5], [4], [2], [1], [1], [5], [2], [1], [0], [5], [3], [4], [3], [2], [2], [2], [1], [3], [5], [3], [0], [5], [5], [3], [3], [5], [2], [1], [1], [4], [0], [2], [0], [0], [2], [1], [3], [2], [5], [3], [1], [1], [4], [5], [5], [2], [3], [3], [2], [0], [3], [1], [5], [4], [1], [0], [4], [0], [5], [2], [3], [4], [2], [5], [0], [2], [2], [1], [5], [1], [5], [5], [1], [0], [4], [3], [4], [2], [1], [3], [5], [0], [0], [1], [2], [1], [5], [4], [4], [2], [3], [4], [2], [0], [3], [2], [2], [3], [3], [3], [4], [4], [4], [5], [1], [4], [2], [0], [3], [5], [4], [0], [0], [1], [3], [1], [5], [4], [3], [5], [0], [2], [1], [4], [3], [1], [1], [1], [4], [0], [5], [4], [2], [0], [4], [0], [1], [1], [1], [1], [4], [5], [4], [1], [0], [4], [1], [3], [5], [0], [1], [3], [3], [0], [0], [3], [2], [2], [2], [4], [2], [4], [1], [3], [2], [2], [1], [0], [0], [5], [4], [5], [3], [3], [4], [3], [3], [5], [5], [0], [5], [5], [4], [2], [3], [3], [4], [2], [0], [1], [3], [5], [4], [4], [5], [1], [5], [1], [1], [1], [3], [3], [0], [5], [5], [2], [3], [0], [3], [1], [1], [4], [4], [3], [3], [0], [5], [5], [0], [1], [3], [4], [4], [5], [2], [4], [2], [4], [0], [1], [5], [2], [5], [2], [4], [0], [3], [5], [4], [5], [2], [0], [0], [2], [1], [5], [4], [2], [1], [2], [2], [4], [4], [2], [1], [0], [1], [3], [5], [0], [4], [3], [2], [0], [1], [2], [1], [0], [4], [4], [2], [1], [1], [2], [5], [5], [1], [5], [2], [3], [2], [0], [4], [1], [1], [0], [5], [3], [5], [3], [3], [3], [0], [2], [3], [0], [2], [1], [3], [5], [1], [5], [4], [0], [4], [1], [4], [1], [2], [2], [4], [4], [3], [1], [1], [4], [3], [4], [5], [5], [2], [2], [5], [4], [4], [5], [5], [0], [5], [3], [3], [0], [3], [2], [2], [2], [0], [3], [5], [4], [1], [4], [3], [5], [5], [2], [3], [5], [0], [2], [1], [3], [2], [5], [4], [0], [5], [4], [1], [1], [5], [4], [0], [2], [1], [0], [2], [4], [4], [2], [0], [1], [2], [0], [2], [2], [0], [4], [2], [4], [4], [2], [3], [3], [1], [2], [0], [0], [2], [1], [5], [3], [3], [3], [0], [5], [2], [2], [5], [2], [3], [3], [4], [5], [0], [0], [3], [3], [2], [1], [3], [3], [2], [4], [2], [4], [5], [0], [5], [0], [5], [2], [5], [3], [3], [4], [1], [3], [3], [5], [5], [3], [5], [5], [1], [4], [4], [4], [0], [2], [1], [1], [5], [4], [0], [3], [0], [5], [1], [2], [0], [5], [4], [0], [0], [2], [2], [0], [1], [0], [1], [1], [1], [0], [1], [5], [2], [2], [4], [3], [1], [3], [2], [3], [4], [3], [2], [5], [1], [0], [4], [0], [5], [1], [5], [5], [1], [3], [5], [4], [4], [3], [0], [3], [5], [2], [4], [4], [1], [2], [0], [0], [5], [3], [1], [5], [4], [5], [0], [1], [1], [3], [2], [3], [4], [4], [5], [1], [3], [4], [1], [1], [5], [3], [2], [1], [2], [5], [0], [0], [0], [2], [4], [1], [3], [3], [1], [3], [3], [0], [2], [4], [1], [1], [4], [5], [4], [3], [4], [1], [2], [5], [2], [4], [2], [1], [1], [5], [0], [5], [3], [5], [2], [2], [4], [1], [1], [2], [2], [2], [4], [3], [2], [0], [2], [2], [5], [4], [5], [3], [0], [1], [4], [2], [1], [1], [4], [4], [1], [3], [0], [3], [4], [1], [4], [4], [4], [0], [1], [3], [2], [3], [1], [4], [5], [4], [0], [2], [3], [2], [4], [3], [4], [1], [2], [5], [4], [2], [1], [5], [4], [0], [5], [1], [4], [2], [0], [1], [5], [5], [0], [5], [3], [4], [2], [3], [4], [2], [0], [5], [3], [2], [1], [3], [1], [0], [0], [0], [2], [0], [1], [5], [3], [3], [5], [5], [2], [4], [5], [3], [3], [2], [0], [5], [0], [2], [3], [4], [3], [3], [1], [2], [4], [4], [2], [0], [1], [3], [0], [0], [1], [2], [5], [3], [4], [2], [1], [3], [3], [2], [5], [1], [0], [3], [1], [2], [5], [3], [2], [0], [1], [4], [3], [5], [4], [0], [2], [2], [4], [1], [1], [0], [5], [5], [3], [3], [5], [4], [5], [5], [1], [2], [1], [3], [4], [0], [1], [1], [5], [4], [1], [5], [5], [1], [1], [0], [0], [5], [3], [5], [4], [3], [5], [2], [5], [2], [1], [2], [1], [3], [0], [1], [5], [0], [3], [5], [4], [3], [5], [1], [3], [3], [0], [0], [0], [5], [4], [2], [5], [3], [2], [0], [3], [1], [4], [5], [0], [5], [2], [4], [2], [3], [3], [4], [2], [1], [0], [5], [2], [4], [1], [3], [1], [4], [0], [4], [1], [4], [4], [2], [4], [4], [4], [4], [5], [2], [0], [0], [4], [1], [5], [2], [2], [2], [4], [4], [3], [0], [1], [5], [1], [1], [2], [2], [2], [1], [2], [2], [1], [2], [2], [3], [2], [5], [4], [3], [3], [5], [6], [5], [6], [5], [4], [3], [4], [3], [4], [4], [4], [5], [6], [3], [6], [6], [5], [6], [3], [5], [3], [5], [6], [6], [4], [4], [1], [4], [2], [2], [2], [3], [4], [2], [2], [4], [3], [4], [2], [3], [3], [1], [2], [3], [3], [1], [1], [2], [3], [3], [5], [4], [5], [4], [4], [4], [3], [2], [6], [3], [5], [1], [5], [6], [2], [3], [2], [2], [6], [2], [6], [1], [3], [2], [5], [1], [5], [3], [2], [6], [6], [5], [3], [2], [3], [5], [1], [1], [3], [6], [6], [1], [4], [5], [5], [2], [4], [5], [4], [6], [4], [6], [5], [6], [2], [3], [4], [1], [1], [1], [2], [5], [5], [2], [6], [2], [6], [5], [5], [3], [4], [2], [6], [1], [2], [4], [6], [6], [2], [4], [4], [3], [2], [2], [4], [6], [1], [6], [5], [2], [1], [1], [3], [4], [6], [6], [5], [5], [3], [5], [1], [2], [4], [1], [2], [5], [6], [4], [6], [4], [3], [3], [5], [4], [6], [4], [3], [6], [5], [5], [5], [6], [6], [3], [4], [2], [6], [4], [4], [5], [5], [3], [5], [3], [4], [5], [3], [4], [4], [2], [6], [4], [3], [5], [3], [6], [1], [2], [1], [5], [6], [6], [3], [6], [5], [3], [4], [6], [6], [4], [3], [6], [6], [4], [4], [5], [5], [2], [6], [5], [6], [2], [4], [1], [3], [2], [2], [4], [4], [6], [2], [5], [3], [3], [5], [4], [5], [3], [2], [3], [2], [1], [2], [5], [4], [1], [5], [6], [3], [3], [4], [4], [1], [6], [4], [6], [1], [1], [4], [3], [4], [1], [2], [5], [5], [3], [3], [3], [1], [4], [2], [2], [6], [3], [5], [3], [3], [6], [5], [5], [4], [3], [2], [5], [2], [2], [4], [2], [5], [6], [6], [5], [2], [4], [3], [4], [6], [2], [5], [5], [2], [4], [3], [4], [2], [4], [5], [2], [6], [5], [3], [1], [2], [3], [2], [2], [3], [4], [1], [4], [1], [1], [3], [3], [4], [2], [3], [1], [1], [4], [5], [1], [2], [5], [6], [3], [3], [6], [3], [4], [4], [6], [4], [6], [3], [3], [2], [2], [4], [6], [1], [3], [2], [4], [1], [5], [3], [2], [2], [1], [2], [2], [5], [4], [6], [6], [3], [6], [2], [6], [6], [5], [2], [2], [6], [5], [5], [2], [6], [6], [6], [1], [2], [1], [1], [6], [5], [1], [5], [5], [6], [1], [2], [4], [5], [5], [2], [6], [6], [3], [5], [5], [4], [3], [5], [5], [3], [6], [3], [4], [5], [6], [2], [3], [4], [4], [4], [4], [6], [5], [3], [4], [5], [6], [4], [3], [6], [6], [5], [3], [5], [4], [5], [4], [6], [3], [2], [4], [4], [6], [3], [5], [5], [2], [2], [2], [4], [4], [6], [5], [1], [6], [4], [1], [3], [2], [2], [4], [6], [6], [5], [2], [5], [5], [6], [1], [1], [3], [3], [1], [3], [6], [3], [3], [2], [4], [3], [2], [5], [4], [3], [2], [6], [4], [2], [3], [4], [5], [1], [1], [3], [3], [5], [6], [2], [2], [5], [1], [3], [2], [1], [4], [2], [5], [4], [4], [5], [3], [2], [1], [3], [5], [4], [5], [5], [2], [5], [1], [1], [3], [2], [4], [3], [1], [2], [6], [1], [2], [5], [3], [4], [5], [6], [4], [3], [3], [3], [2], [6], [6], [5], [5], [2], [2], [6], [4], [3], [4], [1], [5], [2], [6], [4], [6], [4], [5], [4], [3], [5], [6], [6], [3], [2], [5], [5], [4], [1], [2], [3], [6], [4], [4], [5], [2], [6], [6], [4], [2], [4], [5], [5], [2], [4], [3], [3], [3], [3], [2], [5], [1], [4], [1], [6], [2], [3], [6], [6], [2], [4], [1], [3], [1], [4], [2], [6], [5], [4], [2], [5], [6], [2], [5], [3], [3], [1], [1], [1], [6], [3], [6], [2], [2], [3], [3], [4], [2], [6], [6], [4], [4], [2], [3], [3], [4], [3], [1], [6], [4], [6], [4], [2], [1], [6], [3], [3], [1], [3], [2], [6], [6], [4], [3], [2], [5], [3], [5], [4], [6], [2], [2], [1], [1], [2], [6], [5], [2], [1], [4], [3], [5], [3], [6], [4], [4], [3], [6], [2], [2], [5], [6], [5], [1], [2], [2], [2], [5], [5], [6], [5], [6], [1], [1], [6], [3], [5], [2], [2], [1], [3], [3], [4], [2], [5], [4], [4], [6], [6], [2], [5], [6], [5], [3], [1], [6], [3], [5], [3], [2], [6], [1], [6], [4], [4], [4], [1], [2], [5], [2], [6], [6], [4], [2], [4], [6], [1], [5], [6], [1], [1], [1], [6], [5], [5], [6], [4], [2], [5], [5], [6], [4], [2], [5], [5], [4], [2], [3], [3], [5], [5], [4], [3], [4], [4], [6], [2], [5], [6], [5], [6], [4], [5], [6], [4], [3], [4], [3], [1], [6], [6], [3], [1], [6], [2], [1], [5], [4], [3], [3], [3], [1], [5], [5], [3], [2], [1], [4], [6], [3], [3], [3], [3], [4], [6], [2], [6], [2], [2], [5], [5], [2], [4], [4], [4], [6], [4], [5], [6], [5], [2], [4], [6], [6], [3], [5], [5], [4], [2], [5], [3], [2], [6], [4], [6], [3], [5], [3], [4], [3], [6], [6], [2], [4], [5], [2], [3], [1], [2], [5], [4], [2], [3], [5], [3], [2], [5], [3], [6], [5], [6], [3], [4], [3], [3], [4], [1], [2], [6], [1], [6], [6], [5], [2], [1], [2], [4], [5], [6], [4], [5], [1], [6], [4], [2], [3], [6], [3], [4], [3], [1], [1], [3], [4], [2], [6], [5], [3], [2], [6], [2], [3], [3], [4], [5], [2], [2], [1], [4], [1], [5], [2], [4], [1], [6], [4], [5], [5], [2], [3], [6], [5], [5], [4], [4], [5], [6], [5], [1], [3], [6], [4], [1], [3], [3], [1], [4], [2], [2], [5], [1], [4], [4], [2], [4], [4], [3], [3], [5], [5], [5], [2], [6], [6], [4], [2], [3], [1], [5], [3], [2], [3], [2], [5], [4], [2], [5], [5], [6], [6], [6], [3], [3], [2], [1], [4], [2], [1], [5], [1]]'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Save X,Y arrays as lists to json files.\n",
        "\n",
        "x_data = X.tolist() \n",
        "x_path = '/content/drive/My Drive/Colab_Notebooks/X_datanew.json' # FILE SAVE PATH\n",
        "dump(obj = x_data, fp = x_path)\n",
        "\n",
        "y_data = Y.tolist() \n",
        "y_path = '/content/drive/My Drive/Colab_Notebooks/Y_datanew.json' # FILE SAVE PATH\n",
        "dump(obj = y_data, fp = y_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy53CmsODtSQ"
      },
      "outputs": [],
      "source": [
        "# Load X,Y json files back into lists, convert to np.arrays\n",
        "\n",
        "x_path = '/content/drive/My Drive/Colab_Notebooks/X_datanew.json' # FILE LOAD PATH\n",
        "X = load(x_path)\n",
        "X = np.asarray(X, dtype = 'float32')\n",
        "\n",
        "y_path = '/content/drive/My Drive/Colab_Notebooks/Y_datanew.json' # FILE LOAD PATH\n",
        "Y = load(y_path)\n",
        "Y = np.asarray(Y, dtype = 'int8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Photkwq-bw"
      },
      "outputs": [],
      "source": [
        "# Split to train, validation, and test sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_tosplit, y_train, y_tosplit = train_test_split(X, Y, test_size = 0.125, random_state = 1)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_tosplit, y_tosplit, test_size = 0.304, random_state = 1)\n",
        "\n",
        "#'One-hot' vectors for Y: emotion \n",
        "\n",
        "y_train_class = tf.keras.utils.to_categorical(y_train, 7, dtype = 'int8')\n",
        "y_val_class = tf.keras.utils.to_categorical(y_val, 7, dtype = 'int8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgRlZBJbMHvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d3e5c2-fbfa-4059-b990-bd8af2835c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4371, 448, 27)\n",
            "(435, 448, 27)\n",
            "(190, 448, 27)\n"
          ]
        }
      ],
      "source": [
        "# x_train, x_val, and x_test shape check.\n",
        "print(np.shape(x_train))\n",
        "print(np.shape(x_val))\n",
        "print(np.shape(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW4qJ9VogZJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7b8889ff-ae41-46df-c1f3-1376c170ee9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"__ndarray__\": [[6], [4], [4], [0], [5], [2], [5], [0], [1], [6], [2], [3], [6], [5], [2], [4], [3], [4], [2], [4], [2], [0], [1], [2], [5], [2], [6], [1], [6], [2], [1], [5], [1], [5], [4], [4], [5], [1], [0], [0], [5], [2], [1], [1], [1], [2], [3], [5], [1], [1], [3], [1], [1], [0], [3], [0], [4], [4], [6], [2], [4], [0], [6], [1], [1], [6], [0], [3], [1], [6], [4], [5], [6], [5], [0], [2], [3], [3], [3], [1], [1], [5], [4], [1], [0], [0], [0], [2], [0], [5], [5], [2], [4], [5], [3], [4], [5], [6], [2], [1], [6], [2], [3], [1], [2], [1], [4], [4], [2], [4], [4], [6], [3], [5], [4], [2], [0], [4], [5], [3], [1], [3], [5], [1], [2], [4], [2], [0], [1], [4], [1], [1], [2], [0], [1], [3], [2], [2], [2], [3], [3], [5], [2], [2], [0], [0], [0], [5], [2], [3], [2], [0], [1], [4], [1], [5], [2], [2], [0], [3], [6], [3], [3], [1], [3], [1], [1], [4], [5], [5], [3], [2], [4], [2], [2], [1], [4], [6], [1], [4], [3], [3], [2], [4], [0], [5], [4], [5], [3], [3]], \"dtype\": \"int8\", \"shape\": [190, 1], \"Corder\": true}'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Save x_test, y_test to JSON.\n",
        "\n",
        "file_path = 'x_test_data.json'\n",
        "dump(obj = x_test, fp = file_path)\n",
        "\n",
        "file_path = 'y_test_data.json'\n",
        "dump(obj = y_test, fp = file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ROkmISHZCkM"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import callbacks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DroQ71fXxPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294580d5-3a91-4a4f-d3da-86fa49968ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 448, 64)           23552     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57,031\n",
            "Trainable params: 57,031\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/340\n",
            "191/191 [==============================] - 45s 188ms/step - loss: 4.9368 - categorical_accuracy: 0.1668 - val_loss: 4.0880 - val_categorical_accuracy: 0.2138 - lr: 0.0010\n",
            "Epoch 2/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 3.6979 - categorical_accuracy: 0.2450 - val_loss: 4.0438 - val_categorical_accuracy: 0.2299 - lr: 0.0010\n",
            "Epoch 3/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.6900 - categorical_accuracy: 0.2784 - val_loss: 4.0189 - val_categorical_accuracy: 0.1816 - lr: 0.0010\n",
            "Epoch 4/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 3.6097 - categorical_accuracy: 0.3093 - val_loss: 3.9665 - val_categorical_accuracy: 0.2897 - lr: 0.0010\n",
            "Epoch 5/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 3.5967 - categorical_accuracy: 0.3482 - val_loss: 4.2640 - val_categorical_accuracy: 0.3678 - lr: 0.0010\n",
            "Epoch 6/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.6155 - categorical_accuracy: 0.3747 - val_loss: 4.0311 - val_categorical_accuracy: 0.3333 - lr: 0.0010\n",
            "Epoch 7/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.5381 - categorical_accuracy: 0.3935 - val_loss: 3.8934 - val_categorical_accuracy: 0.3218 - lr: 0.0010\n",
            "Epoch 8/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.5516 - categorical_accuracy: 0.3988 - val_loss: 3.7980 - val_categorical_accuracy: 0.3609 - lr: 0.0010\n",
            "Epoch 9/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.6071 - categorical_accuracy: 0.4043 - val_loss: 3.8600 - val_categorical_accuracy: 0.3356 - lr: 0.0010\n",
            "Epoch 10/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.6178 - categorical_accuracy: 0.4063 - val_loss: 3.7912 - val_categorical_accuracy: 0.4046 - lr: 0.0010\n",
            "Epoch 11/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.4869 - categorical_accuracy: 0.4200 - val_loss: 4.3491 - val_categorical_accuracy: 0.3356 - lr: 0.0010\n",
            "Epoch 12/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.5223 - categorical_accuracy: 0.4232 - val_loss: 3.8834 - val_categorical_accuracy: 0.3908 - lr: 0.0010\n",
            "Epoch 13/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 3.5409 - categorical_accuracy: 0.4395 - val_loss: 3.7119 - val_categorical_accuracy: 0.4184 - lr: 0.0010\n",
            "Epoch 14/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.5328 - categorical_accuracy: 0.4482 - val_loss: 4.3580 - val_categorical_accuracy: 0.3333 - lr: 0.0010\n",
            "Epoch 15/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.5199 - categorical_accuracy: 0.4356 - val_loss: 3.8594 - val_categorical_accuracy: 0.3885 - lr: 0.0010\n",
            "Epoch 16/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.5305 - categorical_accuracy: 0.4489 - val_loss: 4.8833 - val_categorical_accuracy: 0.4115 - lr: 0.0010\n",
            "Epoch 17/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 3.5290 - categorical_accuracy: 0.4633 - val_loss: 4.1271 - val_categorical_accuracy: 0.4437 - lr: 0.0010\n",
            "Epoch 18/340\n",
            "191/191 [==============================] - 35s 181ms/step - loss: 3.4581 - categorical_accuracy: 0.4637 - val_loss: 3.8722 - val_categorical_accuracy: 0.4276 - lr: 0.0010\n",
            "Epoch 19/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.4019 - categorical_accuracy: 0.4656 - val_loss: 3.8498 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
            "Epoch 20/340\n",
            "191/191 [==============================] - 35s 181ms/step - loss: 3.4161 - categorical_accuracy: 0.4811 - val_loss: 3.8573 - val_categorical_accuracy: 0.4368 - lr: 0.0010\n",
            "Epoch 21/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.4112 - categorical_accuracy: 0.4656 - val_loss: 4.6568 - val_categorical_accuracy: 0.3057 - lr: 0.0010\n",
            "Epoch 22/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.4644 - categorical_accuracy: 0.4377 - val_loss: 4.0689 - val_categorical_accuracy: 0.4069 - lr: 0.0010\n",
            "Epoch 23/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.3913 - categorical_accuracy: 0.4736 - val_loss: 3.8378 - val_categorical_accuracy: 0.4276 - lr: 0.0010\n",
            "Epoch 24/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.3319 - categorical_accuracy: 0.4807 - val_loss: 4.0443 - val_categorical_accuracy: 0.3977 - lr: 0.0010\n",
            "Epoch 25/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.4863 - categorical_accuracy: 0.4846 - val_loss: 4.0681 - val_categorical_accuracy: 0.4299 - lr: 0.0010\n",
            "Epoch 26/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 3.4572 - categorical_accuracy: 0.4985 - val_loss: 3.8959 - val_categorical_accuracy: 0.4483 - lr: 0.0010\n",
            "Epoch 27/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.3407 - categorical_accuracy: 0.4953 - val_loss: 4.0731 - val_categorical_accuracy: 0.3931 - lr: 0.0010\n",
            "Epoch 28/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.3945 - categorical_accuracy: 0.4987 - val_loss: 4.2751 - val_categorical_accuracy: 0.4023 - lr: 0.0010\n",
            "Epoch 29/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.6060 - categorical_accuracy: 0.4958 - val_loss: 4.4759 - val_categorical_accuracy: 0.4322 - lr: 0.0010\n",
            "Epoch 30/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.4061 - categorical_accuracy: 0.5086 - val_loss: 3.8924 - val_categorical_accuracy: 0.4414 - lr: 0.0010\n",
            "Epoch 31/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 3.3732 - categorical_accuracy: 0.5141 - val_loss: 3.8299 - val_categorical_accuracy: 0.4713 - lr: 0.0010\n",
            "Epoch 32/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.4466 - categorical_accuracy: 0.5056 - val_loss: 3.8004 - val_categorical_accuracy: 0.4690 - lr: 0.0010\n",
            "Epoch 33/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.3878 - categorical_accuracy: 0.5070 - val_loss: 4.3461 - val_categorical_accuracy: 0.4345 - lr: 0.0010\n",
            "Epoch 34/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 3.5072 - categorical_accuracy: 0.5136 - val_loss: 3.7532 - val_categorical_accuracy: 0.4897 - lr: 0.0010\n",
            "Epoch 35/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.3304 - categorical_accuracy: 0.5303 - val_loss: 3.9800 - val_categorical_accuracy: 0.4851 - lr: 0.0010\n",
            "Epoch 36/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.2966 - categorical_accuracy: 0.5253 - val_loss: 4.2472 - val_categorical_accuracy: 0.4874 - lr: 0.0010\n",
            "Epoch 37/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.3644 - categorical_accuracy: 0.5200 - val_loss: 4.0040 - val_categorical_accuracy: 0.4851 - lr: 0.0010\n",
            "Epoch 38/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.3127 - categorical_accuracy: 0.5214 - val_loss: 3.9664 - val_categorical_accuracy: 0.4989 - lr: 0.0010\n",
            "Epoch 39/340\n",
            "191/191 [==============================] - 36s 187ms/step - loss: 3.3531 - categorical_accuracy: 0.5381 - val_loss: 3.7711 - val_categorical_accuracy: 0.5057 - lr: 0.0010\n",
            "Epoch 40/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 3.2795 - categorical_accuracy: 0.5157 - val_loss: 4.2086 - val_categorical_accuracy: 0.4552 - lr: 0.0010\n",
            "Epoch 41/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 3.2612 - categorical_accuracy: 0.5267 - val_loss: 3.8439 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
            "Epoch 42/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.3275 - categorical_accuracy: 0.5342 - val_loss: 3.9265 - val_categorical_accuracy: 0.4966 - lr: 0.0010\n",
            "Epoch 43/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 3.4441 - categorical_accuracy: 0.5216 - val_loss: 3.8651 - val_categorical_accuracy: 0.5218 - lr: 0.0010\n",
            "Epoch 44/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 3.3127 - categorical_accuracy: 0.5328 - val_loss: 4.0554 - val_categorical_accuracy: 0.4943 - lr: 0.0010\n",
            "Epoch 45/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 2.3044 - categorical_accuracy: 0.5209 - val_loss: 2.2444 - val_categorical_accuracy: 0.4644 - lr: 0.0010\n",
            "Epoch 46/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5845 - categorical_accuracy: 0.5315 - val_loss: 1.5106 - val_categorical_accuracy: 0.5103 - lr: 0.0010\n",
            "Epoch 47/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4641 - categorical_accuracy: 0.5427 - val_loss: 1.7108 - val_categorical_accuracy: 0.4322 - lr: 0.0010\n",
            "Epoch 48/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.5578 - categorical_accuracy: 0.5365 - val_loss: 1.6874 - val_categorical_accuracy: 0.5126 - lr: 0.0010\n",
            "Epoch 49/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.4647 - categorical_accuracy: 0.5514 - val_loss: 1.3897 - val_categorical_accuracy: 0.5264 - lr: 0.0010\n",
            "Epoch 50/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4675 - categorical_accuracy: 0.5550 - val_loss: 2.0035 - val_categorical_accuracy: 0.4667 - lr: 0.0010\n",
            "Epoch 51/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5542 - categorical_accuracy: 0.5637 - val_loss: 1.6054 - val_categorical_accuracy: 0.5011 - lr: 0.0010\n",
            "Epoch 52/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.4680 - categorical_accuracy: 0.5514 - val_loss: 1.6473 - val_categorical_accuracy: 0.5172 - lr: 0.0010\n",
            "Epoch 53/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3954 - categorical_accuracy: 0.5658 - val_loss: 1.7886 - val_categorical_accuracy: 0.4989 - lr: 0.0010\n",
            "Epoch 54/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5589 - categorical_accuracy: 0.5621 - val_loss: 1.9957 - val_categorical_accuracy: 0.4736 - lr: 0.0010\n",
            "Epoch 55/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.5380 - categorical_accuracy: 0.5262 - val_loss: 1.4473 - val_categorical_accuracy: 0.5333 - lr: 0.0010\n",
            "Epoch 56/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3647 - categorical_accuracy: 0.5486 - val_loss: 1.8417 - val_categorical_accuracy: 0.4943 - lr: 0.0010\n",
            "Epoch 57/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.4869 - categorical_accuracy: 0.5669 - val_loss: 2.1894 - val_categorical_accuracy: 0.4391 - lr: 0.0010\n",
            "Epoch 58/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3975 - categorical_accuracy: 0.5795 - val_loss: 1.9637 - val_categorical_accuracy: 0.4598 - lr: 0.0010\n",
            "Epoch 59/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4519 - categorical_accuracy: 0.5667 - val_loss: 1.7726 - val_categorical_accuracy: 0.5287 - lr: 0.0010\n",
            "Epoch 60/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3928 - categorical_accuracy: 0.5797 - val_loss: 1.9046 - val_categorical_accuracy: 0.5218 - lr: 0.0010\n",
            "Epoch 61/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.4031 - categorical_accuracy: 0.5871 - val_loss: 1.5409 - val_categorical_accuracy: 0.5011 - lr: 0.0010\n",
            "Epoch 62/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.4265 - categorical_accuracy: 0.5736 - val_loss: 1.7593 - val_categorical_accuracy: 0.5310 - lr: 0.0010\n",
            "Epoch 63/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3226 - categorical_accuracy: 0.5838 - val_loss: 2.3186 - val_categorical_accuracy: 0.4989 - lr: 0.0010\n",
            "Epoch 64/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.4224 - categorical_accuracy: 0.5733 - val_loss: 1.8100 - val_categorical_accuracy: 0.5609 - lr: 0.0010\n",
            "Epoch 65/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.4127 - categorical_accuracy: 0.5905 - val_loss: 2.0455 - val_categorical_accuracy: 0.5333 - lr: 0.0010\n",
            "Epoch 66/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3450 - categorical_accuracy: 0.5946 - val_loss: 2.0062 - val_categorical_accuracy: 0.5218 - lr: 0.0010\n",
            "Epoch 67/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3532 - categorical_accuracy: 0.5941 - val_loss: 1.5551 - val_categorical_accuracy: 0.5149 - lr: 0.0010\n",
            "Epoch 68/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3006 - categorical_accuracy: 0.5921 - val_loss: 1.6386 - val_categorical_accuracy: 0.5287 - lr: 0.0010\n",
            "Epoch 69/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3700 - categorical_accuracy: 0.6044 - val_loss: 1.4842 - val_categorical_accuracy: 0.5655 - lr: 0.0010\n",
            "Epoch 70/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3327 - categorical_accuracy: 0.5967 - val_loss: 1.9479 - val_categorical_accuracy: 0.5701 - lr: 0.0010\n",
            "Epoch 71/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4899 - categorical_accuracy: 0.5859 - val_loss: 1.9779 - val_categorical_accuracy: 0.4943 - lr: 0.0010\n",
            "Epoch 72/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2705 - categorical_accuracy: 0.6058 - val_loss: 1.8541 - val_categorical_accuracy: 0.5425 - lr: 0.0010\n",
            "Epoch 73/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2562 - categorical_accuracy: 0.6056 - val_loss: 1.7187 - val_categorical_accuracy: 0.5471 - lr: 0.0010\n",
            "Epoch 74/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.3818 - categorical_accuracy: 0.6118 - val_loss: 1.4097 - val_categorical_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 75/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2753 - categorical_accuracy: 0.6076 - val_loss: 1.5064 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 76/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3306 - categorical_accuracy: 0.6147 - val_loss: 1.5898 - val_categorical_accuracy: 0.5908 - lr: 0.0010\n",
            "Epoch 77/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5896 - categorical_accuracy: 0.6008 - val_loss: 1.4814 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
            "Epoch 78/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3715 - categorical_accuracy: 0.6108 - val_loss: 1.3194 - val_categorical_accuracy: 0.5977 - lr: 0.0010\n",
            "Epoch 79/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2747 - categorical_accuracy: 0.6189 - val_loss: 1.4539 - val_categorical_accuracy: 0.5724 - lr: 0.0010\n",
            "Epoch 80/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2346 - categorical_accuracy: 0.6111 - val_loss: 1.7164 - val_categorical_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 81/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2870 - categorical_accuracy: 0.6090 - val_loss: 1.4642 - val_categorical_accuracy: 0.5770 - lr: 0.0010\n",
            "Epoch 82/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3010 - categorical_accuracy: 0.6214 - val_loss: 1.3883 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 83/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3503 - categorical_accuracy: 0.6031 - val_loss: 1.8582 - val_categorical_accuracy: 0.5172 - lr: 0.0010\n",
            "Epoch 84/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2883 - categorical_accuracy: 0.6131 - val_loss: 1.4661 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 85/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3265 - categorical_accuracy: 0.6111 - val_loss: 2.3586 - val_categorical_accuracy: 0.5563 - lr: 0.0010\n",
            "Epoch 86/340\n",
            "191/191 [==============================] - 35s 181ms/step - loss: 1.2419 - categorical_accuracy: 0.6356 - val_loss: 1.8396 - val_categorical_accuracy: 0.5816 - lr: 0.0010\n",
            "Epoch 87/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2918 - categorical_accuracy: 0.6019 - val_loss: 1.6761 - val_categorical_accuracy: 0.5540 - lr: 0.0010\n",
            "Epoch 88/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3076 - categorical_accuracy: 0.6266 - val_loss: 1.5846 - val_categorical_accuracy: 0.5908 - lr: 0.0010\n",
            "Epoch 89/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2811 - categorical_accuracy: 0.6307 - val_loss: 2.1398 - val_categorical_accuracy: 0.5517 - lr: 0.0010\n",
            "Epoch 90/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3941 - categorical_accuracy: 0.6255 - val_loss: 1.4518 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 91/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3334 - categorical_accuracy: 0.6305 - val_loss: 1.8776 - val_categorical_accuracy: 0.5149 - lr: 0.0010\n",
            "Epoch 92/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3960 - categorical_accuracy: 0.6298 - val_loss: 2.1958 - val_categorical_accuracy: 0.5448 - lr: 0.0010\n",
            "Epoch 93/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.2740 - categorical_accuracy: 0.6383 - val_loss: 1.4254 - val_categorical_accuracy: 0.6092 - lr: 0.0010\n",
            "Epoch 94/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2726 - categorical_accuracy: 0.6326 - val_loss: 3.4134 - val_categorical_accuracy: 0.3954 - lr: 0.0010\n",
            "Epoch 95/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.3682 - categorical_accuracy: 0.6177 - val_loss: 1.5406 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 96/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3515 - categorical_accuracy: 0.6314 - val_loss: 1.4347 - val_categorical_accuracy: 0.5816 - lr: 0.0010\n",
            "Epoch 97/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4673 - categorical_accuracy: 0.5973 - val_loss: 1.5107 - val_categorical_accuracy: 0.5931 - lr: 0.0010\n",
            "Epoch 98/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3934 - categorical_accuracy: 0.6335 - val_loss: 1.7187 - val_categorical_accuracy: 0.5770 - lr: 0.0010\n",
            "Epoch 99/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.3766 - categorical_accuracy: 0.6372 - val_loss: 1.8590 - val_categorical_accuracy: 0.6184 - lr: 0.0010\n",
            "Epoch 100/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2758 - categorical_accuracy: 0.6367 - val_loss: 1.6740 - val_categorical_accuracy: 0.5862 - lr: 0.0010\n",
            "Epoch 101/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.1533 - categorical_accuracy: 0.6449 - val_loss: 1.2254 - val_categorical_accuracy: 0.5931 - lr: 0.0010\n",
            "Epoch 102/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1919 - categorical_accuracy: 0.6548 - val_loss: 1.6678 - val_categorical_accuracy: 0.5770 - lr: 0.0010\n",
            "Epoch 103/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3251 - categorical_accuracy: 0.6461 - val_loss: 1.4314 - val_categorical_accuracy: 0.5494 - lr: 0.0010\n",
            "Epoch 104/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2703 - categorical_accuracy: 0.6161 - val_loss: 1.7542 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 105/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2135 - categorical_accuracy: 0.6465 - val_loss: 1.7092 - val_categorical_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 106/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2380 - categorical_accuracy: 0.6394 - val_loss: 1.3111 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 107/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2645 - categorical_accuracy: 0.6436 - val_loss: 1.5574 - val_categorical_accuracy: 0.6230 - lr: 0.0010\n",
            "Epoch 108/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2541 - categorical_accuracy: 0.6472 - val_loss: 1.6301 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 109/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2085 - categorical_accuracy: 0.6397 - val_loss: 1.4090 - val_categorical_accuracy: 0.6161 - lr: 0.0010\n",
            "Epoch 110/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2094 - categorical_accuracy: 0.6513 - val_loss: 1.3697 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 111/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.2108 - categorical_accuracy: 0.6552 - val_loss: 1.5884 - val_categorical_accuracy: 0.6345 - lr: 0.0010\n",
            "Epoch 112/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.3357 - categorical_accuracy: 0.6330 - val_loss: 1.4159 - val_categorical_accuracy: 0.6437 - lr: 0.0010\n",
            "Epoch 113/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3025 - categorical_accuracy: 0.6516 - val_loss: 1.8345 - val_categorical_accuracy: 0.5908 - lr: 0.0010\n",
            "Epoch 114/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.1891 - categorical_accuracy: 0.6493 - val_loss: 1.3772 - val_categorical_accuracy: 0.6161 - lr: 0.0010\n",
            "Epoch 115/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.1853 - categorical_accuracy: 0.6603 - val_loss: 2.0991 - val_categorical_accuracy: 0.5632 - lr: 0.0010\n",
            "Epoch 116/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.4372 - categorical_accuracy: 0.6374 - val_loss: 1.5460 - val_categorical_accuracy: 0.6138 - lr: 0.0010\n",
            "Epoch 117/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2755 - categorical_accuracy: 0.6625 - val_loss: 1.3254 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 118/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2781 - categorical_accuracy: 0.6573 - val_loss: 2.2312 - val_categorical_accuracy: 0.5954 - lr: 0.0010\n",
            "Epoch 119/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2265 - categorical_accuracy: 0.6532 - val_loss: 1.6680 - val_categorical_accuracy: 0.6069 - lr: 0.0010\n",
            "Epoch 120/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3110 - categorical_accuracy: 0.6543 - val_loss: 1.9724 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 121/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3688 - categorical_accuracy: 0.6385 - val_loss: 2.4104 - val_categorical_accuracy: 0.5885 - lr: 0.0010\n",
            "Epoch 122/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3532 - categorical_accuracy: 0.6458 - val_loss: 1.6455 - val_categorical_accuracy: 0.5885 - lr: 0.0010\n",
            "Epoch 123/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.1228 - categorical_accuracy: 0.6420 - val_loss: 1.6591 - val_categorical_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 124/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2165 - categorical_accuracy: 0.6660 - val_loss: 1.3546 - val_categorical_accuracy: 0.6207 - lr: 0.0010\n",
            "Epoch 125/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2084 - categorical_accuracy: 0.6527 - val_loss: 2.0068 - val_categorical_accuracy: 0.5471 - lr: 0.0010\n",
            "Epoch 126/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2089 - categorical_accuracy: 0.6575 - val_loss: 2.2031 - val_categorical_accuracy: 0.5724 - lr: 0.0010\n",
            "Epoch 127/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3787 - categorical_accuracy: 0.6312 - val_loss: 1.6283 - val_categorical_accuracy: 0.6161 - lr: 0.0010\n",
            "Epoch 128/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2431 - categorical_accuracy: 0.6394 - val_loss: 1.3932 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 129/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5239 - categorical_accuracy: 0.5832 - val_loss: 1.8574 - val_categorical_accuracy: 0.5425 - lr: 0.0010\n",
            "Epoch 130/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2525 - categorical_accuracy: 0.6413 - val_loss: 1.6627 - val_categorical_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 131/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2477 - categorical_accuracy: 0.6529 - val_loss: 1.4968 - val_categorical_accuracy: 0.6276 - lr: 0.0010\n",
            "Epoch 132/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4673 - categorical_accuracy: 0.6468 - val_loss: 1.8280 - val_categorical_accuracy: 0.6023 - lr: 0.0010\n",
            "Epoch 133/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3112 - categorical_accuracy: 0.6577 - val_loss: 1.4379 - val_categorical_accuracy: 0.6138 - lr: 0.0010\n",
            "Epoch 134/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2932 - categorical_accuracy: 0.6360 - val_loss: 1.4059 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
            "Epoch 135/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2882 - categorical_accuracy: 0.6381 - val_loss: 1.5875 - val_categorical_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 136/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5518 - categorical_accuracy: 0.6259 - val_loss: 2.3195 - val_categorical_accuracy: 0.4920 - lr: 0.0010\n",
            "Epoch 137/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.6572 - categorical_accuracy: 0.6166 - val_loss: 1.5384 - val_categorical_accuracy: 0.5632 - lr: 0.0010\n",
            "Epoch 138/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2239 - categorical_accuracy: 0.6330 - val_loss: 1.6253 - val_categorical_accuracy: 0.5931 - lr: 0.0010\n",
            "Epoch 139/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.4259 - categorical_accuracy: 0.6333 - val_loss: 2.0370 - val_categorical_accuracy: 0.5885 - lr: 0.0010\n",
            "Epoch 140/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3278 - categorical_accuracy: 0.6248 - val_loss: 1.5593 - val_categorical_accuracy: 0.6299 - lr: 0.0010\n",
            "Epoch 141/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2996 - categorical_accuracy: 0.6536 - val_loss: 1.7083 - val_categorical_accuracy: 0.5908 - lr: 0.0010\n",
            "Epoch 142/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2714 - categorical_accuracy: 0.6365 - val_loss: 1.9596 - val_categorical_accuracy: 0.5540 - lr: 0.0010\n",
            "Epoch 143/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3540 - categorical_accuracy: 0.6291 - val_loss: 2.0841 - val_categorical_accuracy: 0.5149 - lr: 0.0010\n",
            "Epoch 144/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5549 - categorical_accuracy: 0.6076 - val_loss: 1.7869 - val_categorical_accuracy: 0.5862 - lr: 0.0010\n",
            "Epoch 145/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.6322 - categorical_accuracy: 0.5900 - val_loss: 1.8512 - val_categorical_accuracy: 0.5678 - lr: 0.0010\n",
            "Epoch 146/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1837 - categorical_accuracy: 0.6516 - val_loss: 1.4371 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
            "Epoch 147/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2362 - categorical_accuracy: 0.6484 - val_loss: 1.5926 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 148/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2633 - categorical_accuracy: 0.6738 - val_loss: 1.4041 - val_categorical_accuracy: 0.5908 - lr: 0.0010\n",
            "Epoch 149/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.5440 - categorical_accuracy: 0.6159 - val_loss: 1.5742 - val_categorical_accuracy: 0.5954 - lr: 0.0010\n",
            "Epoch 150/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3915 - categorical_accuracy: 0.6273 - val_loss: 1.5466 - val_categorical_accuracy: 0.6069 - lr: 0.0010\n",
            "Epoch 151/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2627 - categorical_accuracy: 0.6399 - val_loss: 1.7997 - val_categorical_accuracy: 0.5425 - lr: 0.0010\n",
            "Epoch 152/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2666 - categorical_accuracy: 0.6420 - val_loss: 2.4834 - val_categorical_accuracy: 0.5701 - lr: 0.0010\n",
            "Epoch 153/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3814 - categorical_accuracy: 0.6287 - val_loss: 1.6619 - val_categorical_accuracy: 0.5816 - lr: 0.0010\n",
            "Epoch 154/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4351 - categorical_accuracy: 0.6532 - val_loss: 1.2911 - val_categorical_accuracy: 0.6345 - lr: 0.0010\n",
            "Epoch 155/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2748 - categorical_accuracy: 0.6440 - val_loss: 2.2998 - val_categorical_accuracy: 0.5080 - lr: 0.0010\n",
            "Epoch 156/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3300 - categorical_accuracy: 0.6280 - val_loss: 1.3776 - val_categorical_accuracy: 0.6414 - lr: 0.0010\n",
            "Epoch 157/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3921 - categorical_accuracy: 0.6532 - val_loss: 1.6393 - val_categorical_accuracy: 0.6391 - lr: 0.0010\n",
            "Epoch 158/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4102 - categorical_accuracy: 0.6312 - val_loss: 2.3170 - val_categorical_accuracy: 0.5655 - lr: 0.0010\n",
            "Epoch 159/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3765 - categorical_accuracy: 0.6456 - val_loss: 1.5071 - val_categorical_accuracy: 0.6023 - lr: 0.0010\n",
            "Epoch 160/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3604 - categorical_accuracy: 0.6312 - val_loss: 1.5687 - val_categorical_accuracy: 0.6345 - lr: 0.0010\n",
            "Epoch 161/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3287 - categorical_accuracy: 0.6564 - val_loss: 1.6387 - val_categorical_accuracy: 0.6023 - lr: 0.0010\n",
            "Epoch 162/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2622 - categorical_accuracy: 0.6577 - val_loss: 2.1113 - val_categorical_accuracy: 0.5839 - lr: 0.0010\n",
            "Epoch 163/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.4026 - categorical_accuracy: 0.6552 - val_loss: 1.5764 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 164/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2232 - categorical_accuracy: 0.6621 - val_loss: 2.0755 - val_categorical_accuracy: 0.5885 - lr: 0.0010\n",
            "Epoch 165/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.3028 - categorical_accuracy: 0.6632 - val_loss: 1.4897 - val_categorical_accuracy: 0.6322 - lr: 0.0010\n",
            "Epoch 166/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.1541 - categorical_accuracy: 0.6738 - val_loss: 1.3087 - val_categorical_accuracy: 0.6575 - lr: 0.0010\n",
            "Epoch 167/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2036 - categorical_accuracy: 0.6381 - val_loss: 1.7086 - val_categorical_accuracy: 0.5724 - lr: 0.0010\n",
            "Epoch 168/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1485 - categorical_accuracy: 0.6637 - val_loss: 1.9323 - val_categorical_accuracy: 0.6207 - lr: 0.0010\n",
            "Epoch 169/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2518 - categorical_accuracy: 0.6653 - val_loss: 1.3909 - val_categorical_accuracy: 0.6184 - lr: 0.0010\n",
            "Epoch 170/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3416 - categorical_accuracy: 0.6564 - val_loss: 1.5927 - val_categorical_accuracy: 0.6207 - lr: 0.0010\n",
            "Epoch 171/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2360 - categorical_accuracy: 0.6543 - val_loss: 1.6066 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 172/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2039 - categorical_accuracy: 0.6616 - val_loss: 1.4015 - val_categorical_accuracy: 0.6506 - lr: 0.0010\n",
            "Epoch 173/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2257 - categorical_accuracy: 0.6678 - val_loss: 1.6866 - val_categorical_accuracy: 0.6368 - lr: 0.0010\n",
            "Epoch 174/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3908 - categorical_accuracy: 0.6621 - val_loss: 1.5703 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 175/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3007 - categorical_accuracy: 0.6358 - val_loss: 1.4915 - val_categorical_accuracy: 0.6207 - lr: 0.0010\n",
            "Epoch 176/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2591 - categorical_accuracy: 0.6651 - val_loss: 1.5304 - val_categorical_accuracy: 0.6230 - lr: 0.0010\n",
            "Epoch 177/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2967 - categorical_accuracy: 0.6726 - val_loss: 1.5237 - val_categorical_accuracy: 0.6414 - lr: 0.0010\n",
            "Epoch 178/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1989 - categorical_accuracy: 0.6607 - val_loss: 1.3467 - val_categorical_accuracy: 0.6299 - lr: 0.0010\n",
            "Epoch 179/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1981 - categorical_accuracy: 0.6646 - val_loss: 1.6847 - val_categorical_accuracy: 0.5563 - lr: 0.0010\n",
            "Epoch 180/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.2099 - categorical_accuracy: 0.6813 - val_loss: 1.3255 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 181/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2359 - categorical_accuracy: 0.6642 - val_loss: 1.6017 - val_categorical_accuracy: 0.6230 - lr: 0.0010\n",
            "Epoch 182/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: nan - categorical_accuracy: 0.6646 - val_loss: 1.6006 - val_categorical_accuracy: 0.6161 - lr: 0.0010\n",
            "Epoch 183/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2830 - categorical_accuracy: 0.6580 - val_loss: 1.7893 - val_categorical_accuracy: 0.6230 - lr: 0.0010\n",
            "Epoch 184/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1221 - categorical_accuracy: 0.6754 - val_loss: 1.5478 - val_categorical_accuracy: 0.6483 - lr: 0.0010\n",
            "Epoch 185/340\n",
            "191/191 [==============================] - 35s 182ms/step - loss: 1.4205 - categorical_accuracy: 0.6671 - val_loss: 1.9604 - val_categorical_accuracy: 0.5816 - lr: 0.0010\n",
            "Epoch 186/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.2514 - categorical_accuracy: 0.6747 - val_loss: 1.8653 - val_categorical_accuracy: 0.6506 - lr: 0.0010\n",
            "Epoch 187/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1563 - categorical_accuracy: 0.6861 - val_loss: 1.4149 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
            "Epoch 188/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1407 - categorical_accuracy: 0.6877 - val_loss: 1.4698 - val_categorical_accuracy: 0.6322 - lr: 0.0010\n",
            "Epoch 189/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.4647 - categorical_accuracy: 0.6394 - val_loss: 1.6935 - val_categorical_accuracy: 0.6575 - lr: 0.0010\n",
            "Epoch 190/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: nan - categorical_accuracy: 0.6792 - val_loss: 1.2948 - val_categorical_accuracy: 0.6092 - lr: 0.0010\n",
            "Epoch 191/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2637 - categorical_accuracy: 0.6600 - val_loss: 1.5062 - val_categorical_accuracy: 0.6276 - lr: 0.0010\n",
            "Epoch 192/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3904 - categorical_accuracy: 0.6525 - val_loss: 1.5708 - val_categorical_accuracy: 0.6345 - lr: 0.0010\n",
            "Epoch 193/340\n",
            "191/191 [==============================] - 35s 183ms/step - loss: 1.3225 - categorical_accuracy: 0.6706 - val_loss: 1.8618 - val_categorical_accuracy: 0.6322 - lr: 0.0010\n",
            "Epoch 194/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2767 - categorical_accuracy: 0.6500 - val_loss: 1.3599 - val_categorical_accuracy: 0.6322 - lr: 0.0010\n",
            "Epoch 195/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2238 - categorical_accuracy: 0.6806 - val_loss: 1.4302 - val_categorical_accuracy: 0.6460 - lr: 0.0010\n",
            "Epoch 196/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1852 - categorical_accuracy: 0.6772 - val_loss: 1.5573 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 197/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.1242 - categorical_accuracy: 0.6749 - val_loss: 2.1116 - val_categorical_accuracy: 0.5747 - lr: 0.0010\n",
            "Epoch 198/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3613 - categorical_accuracy: 0.6678 - val_loss: 1.7322 - val_categorical_accuracy: 0.6138 - lr: 0.0010\n",
            "Epoch 199/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2309 - categorical_accuracy: 0.6605 - val_loss: 1.6549 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 200/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2596 - categorical_accuracy: 0.6774 - val_loss: 1.7020 - val_categorical_accuracy: 0.6138 - lr: 0.0010\n",
            "Epoch 201/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6683 - val_loss: 1.9090 - val_categorical_accuracy: 0.6460 - lr: 0.0010\n",
            "Epoch 202/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2834 - categorical_accuracy: 0.6786 - val_loss: 1.4768 - val_categorical_accuracy: 0.6506 - lr: 0.0010\n",
            "Epoch 203/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2025 - categorical_accuracy: 0.6674 - val_loss: 1.4870 - val_categorical_accuracy: 0.6483 - lr: 0.0010\n",
            "Epoch 204/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2172 - categorical_accuracy: 0.6733 - val_loss: 1.7223 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 205/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2068 - categorical_accuracy: 0.6683 - val_loss: 1.8136 - val_categorical_accuracy: 0.6138 - lr: 0.0010\n",
            "Epoch 206/340\n",
            "191/191 [==============================] - 36s 187ms/step - loss: 1.3270 - categorical_accuracy: 0.6696 - val_loss: 1.2785 - val_categorical_accuracy: 0.6644 - lr: 0.0010\n",
            "Epoch 207/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.1745 - categorical_accuracy: 0.6795 - val_loss: 1.9737 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 208/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.4227 - categorical_accuracy: 0.6717 - val_loss: 2.4154 - val_categorical_accuracy: 0.5310 - lr: 0.0010\n",
            "Epoch 209/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6580 - val_loss: 1.7698 - val_categorical_accuracy: 0.6253 - lr: 0.0010\n",
            "Epoch 210/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6744 - val_loss: 1.4739 - val_categorical_accuracy: 0.6276 - lr: 0.0010\n",
            "Epoch 211/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.3467 - categorical_accuracy: 0.6676 - val_loss: 1.6716 - val_categorical_accuracy: 0.6299 - lr: 0.0010\n",
            "Epoch 212/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.2762 - categorical_accuracy: 0.6612 - val_loss: 1.8217 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 213/340\n",
            "191/191 [==============================] - 36s 186ms/step - loss: 1.3402 - categorical_accuracy: 0.6646 - val_loss: 1.4434 - val_categorical_accuracy: 0.6138 - lr: 0.0010\n",
            "Epoch 214/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: nan - categorical_accuracy: 0.6754 - val_loss: 1.7699 - val_categorical_accuracy: 0.6207 - lr: 0.0010\n",
            "Epoch 215/340\n",
            "191/191 [==============================] - 36s 186ms/step - loss: 1.2882 - categorical_accuracy: 0.6726 - val_loss: 1.7687 - val_categorical_accuracy: 0.6483 - lr: 0.0010\n",
            "Epoch 216/340\n",
            "191/191 [==============================] - 36s 186ms/step - loss: 1.4534 - categorical_accuracy: 0.6619 - val_loss: 1.5758 - val_categorical_accuracy: 0.6299 - lr: 0.0010\n",
            "Epoch 217/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.3649 - categorical_accuracy: 0.6667 - val_loss: 2.0763 - val_categorical_accuracy: 0.6069 - lr: 0.0010\n",
            "Epoch 218/340\n",
            "191/191 [==============================] - 36s 187ms/step - loss: 1.5859 - categorical_accuracy: 0.6474 - val_loss: 1.5074 - val_categorical_accuracy: 0.6069 - lr: 0.0010\n",
            "Epoch 219/340\n",
            "191/191 [==============================] - 36s 186ms/step - loss: 1.3691 - categorical_accuracy: 0.6230 - val_loss: 1.4585 - val_categorical_accuracy: 0.5793 - lr: 0.0010\n",
            "Epoch 220/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2442 - categorical_accuracy: 0.6358 - val_loss: 1.2829 - val_categorical_accuracy: 0.6184 - lr: 0.0010\n",
            "Epoch 221/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.1964 - categorical_accuracy: 0.6765 - val_loss: 1.4848 - val_categorical_accuracy: 0.6391 - lr: 0.0010\n",
            "Epoch 222/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6747 - val_loss: 1.5208 - val_categorical_accuracy: 0.6483 - lr: 0.0010\n",
            "Epoch 223/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.1574 - categorical_accuracy: 0.6820 - val_loss: 1.6268 - val_categorical_accuracy: 0.6115 - lr: 0.0010\n",
            "Epoch 224/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.1656 - categorical_accuracy: 0.6738 - val_loss: 1.6399 - val_categorical_accuracy: 0.6437 - lr: 0.0010\n",
            "Epoch 225/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2232 - categorical_accuracy: 0.6788 - val_loss: 2.1323 - val_categorical_accuracy: 0.6092 - lr: 0.0010\n",
            "Epoch 226/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6813 - val_loss: 1.4889 - val_categorical_accuracy: 0.6023 - lr: 0.0010\n",
            "Epoch 227/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3901 - categorical_accuracy: 0.6747 - val_loss: 1.3668 - val_categorical_accuracy: 0.6322 - lr: 0.0010\n",
            "Epoch 228/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2279 - categorical_accuracy: 0.6728 - val_loss: 1.5173 - val_categorical_accuracy: 0.6299 - lr: 0.0010\n",
            "Epoch 229/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: nan - categorical_accuracy: 0.6847 - val_loss: 1.6496 - val_categorical_accuracy: 0.6529 - lr: 0.0010\n",
            "Epoch 230/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6669 - val_loss: nan - val_categorical_accuracy: 0.6184 - lr: 0.0010\n",
            "Epoch 231/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2695 - categorical_accuracy: 0.6850 - val_loss: nan - val_categorical_accuracy: 0.6184 - lr: 0.0010\n",
            "Epoch 232/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.2545 - categorical_accuracy: 0.6825 - val_loss: nan - val_categorical_accuracy: 0.6506 - lr: 0.0010\n",
            "Epoch 233/340\n",
            "191/191 [==============================] - 36s 187ms/step - loss: nan - categorical_accuracy: 0.6811 - val_loss: 1.3562 - val_categorical_accuracy: 0.6690 - lr: 0.0010\n",
            "Epoch 234/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: nan - categorical_accuracy: 0.6806 - val_loss: 1.5832 - val_categorical_accuracy: 0.6552 - lr: 0.0010\n",
            "Epoch 235/340\n",
            "191/191 [==============================] - 36s 186ms/step - loss: nan - categorical_accuracy: 0.6749 - val_loss: 1.2828 - val_categorical_accuracy: 0.6575 - lr: 0.0010\n",
            "Epoch 236/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.2578 - categorical_accuracy: 0.6884 - val_loss: 1.7691 - val_categorical_accuracy: 0.6598 - lr: 0.0010\n",
            "Epoch 237/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6664 - val_loss: 1.7617 - val_categorical_accuracy: 0.5954 - lr: 0.0010\n",
            "Epoch 238/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.4027 - categorical_accuracy: 0.6500 - val_loss: 2.0349 - val_categorical_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 239/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6822 - val_loss: 1.3792 - val_categorical_accuracy: 0.6437 - lr: 0.0010\n",
            "Epoch 240/340\n",
            "191/191 [==============================] - 36s 186ms/step - loss: 1.3460 - categorical_accuracy: 0.6756 - val_loss: 1.4565 - val_categorical_accuracy: 0.6345 - lr: 0.0010\n",
            "Epoch 241/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.1771 - categorical_accuracy: 0.6815 - val_loss: 1.4469 - val_categorical_accuracy: 0.6575 - lr: 0.0010\n",
            "Epoch 242/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: nan - categorical_accuracy: 0.6873 - val_loss: 1.8660 - val_categorical_accuracy: 0.6368 - lr: 0.0010\n",
            "Epoch 243/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.3123 - categorical_accuracy: 0.6866 - val_loss: 1.8422 - val_categorical_accuracy: 0.5977 - lr: 0.0010\n",
            "Epoch 244/340\n",
            "191/191 [==============================] - 36s 186ms/step - loss: 1.2534 - categorical_accuracy: 0.6825 - val_loss: 1.3786 - val_categorical_accuracy: 0.6391 - lr: 0.0010\n",
            "Epoch 245/340\n",
            "191/191 [==============================] - 36s 188ms/step - loss: 1.2885 - categorical_accuracy: 0.6909 - val_loss: 1.6002 - val_categorical_accuracy: 0.6759 - lr: 0.0010\n",
            "Epoch 246/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.3900 - categorical_accuracy: 0.6706 - val_loss: 1.4278 - val_categorical_accuracy: 0.6529 - lr: 0.0010\n",
            "Epoch 247/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: 1.2377 - categorical_accuracy: 0.6889 - val_loss: 1.3525 - val_categorical_accuracy: 0.6759 - lr: 0.0010\n",
            "Epoch 248/340\n",
            "191/191 [==============================] - 35s 184ms/step - loss: 1.3918 - categorical_accuracy: 0.6863 - val_loss: 2.0807 - val_categorical_accuracy: 0.6414 - lr: 0.0010\n",
            "Epoch 249/340\n",
            "191/191 [==============================] - 35s 185ms/step - loss: nan - categorical_accuracy: 0.6792 - val_loss: 1.5197 - val_categorical_accuracy: 0.6138 - lr: 0.0010\n",
            "Epoch 250/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.3714 - categorical_accuracy: 0.6850 - val_loss: 1.6101 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
            "Epoch 251/340\n",
            "191/191 [==============================] - 35s 186ms/step - loss: 1.8258 - categorical_accuracy: 0.6566 - val_loss: 2.1620 - val_categorical_accuracy: 0.6667 - lr: 0.0010\n",
            "Epoch 252/340\n",
            " 16/191 [=>............................] - ETA: 31s - loss: 1.9003 - categorical_accuracy: 0.6766"
          ]
        }
      ],
      "source": [
        "# Initializing the model\n",
        "\n",
        "model = Sequential() #relu , siqmoid , software\n",
        "model.add(layers.LSTM(64, return_sequences = True, input_shape=(X.shape[1:3])))\n",
        "model.add(layers.LSTM(64))\n",
        "model.add(layers.Dense(7, activation = 'relu'))\n",
        "\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "batch_size = 23\n",
        "\n",
        "# Callbacks functions\n",
        "checkpoint_path = '/content/drive/My Drive/Colab_Notebooks/best_weights.hdf5'\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "\n",
        "#-> Save the best weights\n",
        "mcp_save = callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True,\n",
        "                           monitor='val_categorical_accuracy',\n",
        "                           mode='max')\n",
        "#-> Reduce learning rate after 100 epoches without improvement.\n",
        "rlrop = callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy', \n",
        "                                    factor=0.1, patience=100)\n",
        "                             \n",
        "# Compile & train   \n",
        "model.compile(loss='categorical_crossentropy', \n",
        "                optimizer='RMSProp', \n",
        "                metrics=['categorical_accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train_class, #shuffle= True, btsh2lb al data\n",
        "                      epochs=340, batch_size = batch_size,   #340   \n",
        "                      validation_data = (x_val, y_val_class), \n",
        "                      callbacks = [mcp_save, rlrop])\n",
        "# Define the best weights to the model.\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "print(f\"Running time: {(toc - tic)/60:0.4f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrq5r7wpufzS"
      },
      "outputs": [],
      "source": [
        "# checkpoint_path = '/content/drive/My Drive/Colab_Notebooks/best_weights.hdf5'\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(layers.LSTM(64, return_sequences = True, input_shape=(X.shape[1:3])))\n",
        "# model.add(layers.LSTM(64))\n",
        "# model.add(layers.Dense(7, activation = 'softmax'))\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy', \n",
        "#                 optimizer='RMSProp', \n",
        "#                 metrics=['categorical_accuracy'])\n",
        "\n",
        "\n",
        "# model.load_weights(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3xnV1S7ra_Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu3eLmdtNlCF"
      },
      "outputs": [],
      "source": [
        "# Loss, Accuracy presentation\n",
        "\n",
        "# Plot history: Loss\n",
        "plt.plot(history.history['loss'], label='Loss (training data)')\n",
        "plt.plot(history.history['val_loss'], label='Loss (validation data)')\n",
        "plt.title('Loss for train and validation')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "#Plot history: Accuracy\n",
        "plt.plot(history.history['categorical_accuracy'], label='Acc (training data)')\n",
        "plt.plot(history.history['val_categorical_accuracy'], label='Acc (validation data)')\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Acc %')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc6qcBGOMHv7"
      },
      "outputs": [],
      "source": [
        "# Validation score\n",
        "loss,acc = model.evaluate(x_val, y_val_class, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhR-4z86VPCy"
      },
      "outputs": [],
      "source": [
        "y_val_class.shape\n",
        "x_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Icd1yFHra_Z"
      },
      "outputs": [],
      "source": [
        "# Validation Confusion matrix\n",
        "\n",
        "y_val_class = np.argmax(y_val_class, axis=1)\n",
        "predictions = model.predict(x_val)\n",
        "y_pred_class = np.argmax(predictions, axis=1)\n",
        "cm=confusion_matrix(y_val_class, y_pred_class)\n",
        "\n",
        "# index = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "# columns = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "\n",
        "index = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "columns = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "\n",
        "cm_df = pd.DataFrame(cm,index,columns)\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = plt.axes()\n",
        "\n",
        "sns.heatmap(cm_df, ax = ax, cmap = 'PuBu', fmt=\"d\", annot=True) \n",
        "ax.set_ylabel('True emotion')\n",
        "ax.set_xlabel('Predicted emotion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j70DNePpra_c"
      },
      "outputs": [],
      "source": [
        "# Validation set prediction accuracy rates\n",
        "\n",
        "values = cm.diagonal()\n",
        "print(values) \n",
        "row_sum = np.sum(cm,axis=1)\n",
        "print(row_sum)\n",
        "acc = values / row_sum\n",
        "\n",
        "print('Validation set predicted emotions accuracy:')\n",
        "for e in range(0, len(values)):\n",
        "    print(index[e],':', f\"{(acc[e]):0.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ybMruIrra_d"
      },
      "outputs": [],
      "source": [
        "# Saving model & weights\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "model_json = model.to_json()\n",
        "saved_model_path = '/content/drive/My Drive/Colab_Notebooks/model8723.json'\n",
        "saved_weights_path = '/content/drive/My Drive/Colab_Notebooks/model8723_weights.h5'\n",
        "\n",
        "\n",
        "with open(saved_model_path, \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "model.save_weights(saved_weights_path)\n",
        "print(\"Saved model to disk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CvsDF6cOa3z"
      },
      "outputs": [],
      "source": [
        "# Reading the model from JSON file\n",
        "\n",
        "saved_model_path = '/content/drive/MyDrive/Colab_Notebooks/model8723.json'\n",
        "saved_weights_path = '/content/drive/MyDrive/Colab_Notebooks/model8723_weights.h5'\n",
        "\n",
        "with open(saved_model_path , 'r') as json_file:\n",
        "    json_savedModel = json_file.read()\n",
        "    \n",
        "# Loading the model architecture, weights\n",
        "model = tf.keras.models.model_from_json(json_savedModel)\n",
        "model.load_weights(saved_weights_path)\n",
        "\n",
        "# Compiling the model with similar parameters as the original model.\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "                optimizer='RMSProp', \n",
        "                metrics=['categorical_accuracy'])\n",
        "\n",
        "# Model's structure visualization\n",
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aw0_8VfOa30"
      },
      "outputs": [],
      "source": [
        "# Loading x_test, y_test json files, and converting to np.arrays\n",
        "\n",
        "x_test = load( 'x_test_data.json')\n",
        "x_test = np.asarray(x_test).astype('float32')\n",
        "\n",
        "y_test = load('y_test_data.json')\n",
        "y_test = np.asarray(y_test).astype('int8')\n",
        "\n",
        "y_test_class = tf.keras.utils.to_categorical(y_test, 7, dtype = 'int8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b56-z_GUOa31"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(x_test, y_test_class, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ssa4HRVOa31"
      },
      "outputs": [],
      "source": [
        "# Test set Confusion matrix\n",
        "print(x_test.shape)\n",
        "y = np.argmax(y_test_class, axis=1)\n",
        "predictions = model.predict(x_test)\n",
        "y_pred_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "cm=confusion_matrix(y, y_pred_class)\n",
        "\n",
        "# index = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "# columns = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "\n",
        "index = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "columns = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']  \n",
        "\n",
        " \n",
        "cm_df = pd.DataFrame(cm,index,columns)                      \n",
        "plt.figure(figsize=(12,7))\n",
        "ax = plt.axes()\n",
        "\n",
        "sns.heatmap(cm_df, ax = ax, cmap = 'BuGn', fmt=\"d\", annot=True)\n",
        "ax.set_ylabel('True emotion')\n",
        "ax.set_xlabel('Predicted emotion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwbkmv2yOa32"
      },
      "outputs": [],
      "source": [
        "  # Test set prediction accuracy rates\n",
        "\n",
        "values = cm.diagonal()\n",
        "row_sum = np.sum(cm,axis=1)\n",
        "acc = values / row_sum\n",
        "\n",
        "print('Test set predicted emotions accuracy:')\n",
        "for e in range(0, len(values)):\n",
        "    print(index[e],':', f\"{(acc[e]):0.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmHZYHVErkQ2"
      },
      "outputs": [],
      "source": [
        "subdir = '/content/drive/MyDrive/AudioFiles (1)/Used Dataset'\n",
        "traning = [\"03-01-07-02-01-01-23.wav\", \"1013_TSI_SAD_XX.wav\", \"YAF_young_happy.wav\"]\n",
        "\n",
        "# subdir = \"/content/drive/MyDrive/Colab_Notebooks/AudioFiles\"\n",
        "# traning = [\"ode.wav\"]\n",
        "\n",
        "for file in traning:\n",
        "    rms = []\n",
        "    zcr = []\n",
        "    mfcc = []\n",
        "    chroma = []\n",
        "    # emotions = []\n",
        "\n",
        "    # Initialize variables\n",
        "    total_length = 228864 #228864  #305152  #5005152    # desired frame length for all of the audio samples.\n",
        "    frame_length = 2048\n",
        "    hop_length = 512\n",
        "\n",
        "\n",
        "    # Fetch the sample rate.\n",
        "    _, sr = librosa.load(path = os.path.join(subdir,file), sr = None) # sr (the sample rate) is used for librosa's MFCCs. '_' is irrelevant.\n",
        "    # Load the audio file.\n",
        "    rawsound = AudioSegment.from_file(os.path.join(subdir,file)) \n",
        "    # Normalize the audio to +5.0 dBFS.\n",
        "    normalizedsound = effects.normalize(rawsound, headroom = 5.0) \n",
        "    # Transform the normalized audio to np.array of samples.\n",
        "    normal_x = np.array(normalizedsound.get_array_of_samples(), dtype = 'float32')\n",
        "    # Trim silence from the beginning and the end.\n",
        "    xt,  index = librosa.effects.trim(normal_x, top_db=30)\n",
        "    # Pad for duration equalization.\n",
        "    # print(xt.shape)\n",
        "    padded_x = np.pad(xt, (0, total_length-len(xt)), 'constant')\n",
        "    # Noise reduction.\n",
        "    final_x = nr.reduce_noise(y=padded_x,y_noise=padded_x, sr=sr)\n",
        "\n",
        "    # Features extraction \n",
        "    f1 = librosa.feature.rms(final_x, frame_length=frame_length, hop_length=hop_length) # Energy - Root Mean Square\n",
        "    f2 = librosa.feature.zero_crossing_rate(final_x , frame_length=frame_length, hop_length=hop_length, center=True) # ZCR\n",
        "    f3 = librosa.feature.mfcc(final_x, sr=sr, n_mfcc=13, hop_length = hop_length) # MFCC\n",
        "    f4 = librosa.feature.chroma_stft(final_x,sr=sr) #chroma\n",
        "      \n",
        "\n",
        "    # Emotion extraction from the different databases\n",
        "    # if (find_emotion_T(file) != \"-1\"): #TESS database validation\n",
        "    #       name = find_emotion_T(file)\n",
        "    # else:                              #RAVDESS database validation\n",
        "    #       name = file[6:8]                      \n",
        "    # print(name);\n",
        "    # Filling the data lists  \n",
        "    rms.append(f1)\n",
        "    zcr.append(f2)\n",
        "    mfcc.append(f3)\n",
        "    chroma.append(f4)\n",
        "    # emotions.append((name)) \n",
        "\n",
        "    # Adjusting features shape to the 3D format: (batch, timesteps, feature)\n",
        "\n",
        "    f_rms = np.asarray(rms).astype('float32')\n",
        "    f_rms = np.swapaxes(f_rms,1,2)\n",
        "    f_zcr = np.asarray(zcr).astype('float32')\n",
        "    f_zcr = np.swapaxes(f_zcr,1,2)\n",
        "    f_mfccs = np.asarray(mfcc).astype('float32')\n",
        "    f_mfccs = np.swapaxes(f_mfccs,1,2)\n",
        "    f_chroma = np.asarray(chroma).astype('float32')\n",
        "    f_chroma = np.swapaxes(f_chroma,1,2)\n",
        "\n",
        "    # Concatenating all features to 'X' variable.\n",
        "    X = np.concatenate(( f_rms,f_zcr,f_mfccs,  f_chroma), axis=2) #,\n",
        "\n",
        "    # Preparing 'Y' as a 2D shaped variable.\n",
        "    # Y = np.asarray(emotions).astype('int8')\n",
        "    # Y = np.expand_dims(Y, axis=1)\n",
        "\n",
        "\n",
        "    # y = np.argmax(Y, axis=1)\n",
        "    predictions = model.predict(X)\n",
        "    # y_pred_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "    emotions = {\n",
        "        0 : 'neutral',\n",
        "        1 : 'happy',\n",
        "        2 : 'sad',\n",
        "        3 : 'angry',\n",
        "        4 : 'fearful',  \n",
        "        5 : 'disgust',\n",
        "        6 : 'suprised'   \n",
        "    }  \n",
        "    # emo_list = list(emotions.values())\n",
        "\n",
        "    max_emo = np.argmax(predictions)\n",
        "    print('max emotion:', emotions.get(max_emo,-1))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GD 2_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}